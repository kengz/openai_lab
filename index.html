<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>OpenAI Lab Doc</title>

    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight {
  color: #faf6e4;
  background-color: #122b3b;
}
.highlight .gl {
  color: #dee5e7;
  background-color: #4e5d62;
}
.highlight .c, .highlight .cd, .highlight .cm, .highlight .c1, .highlight .cs {
  color: #6c8b9f;
  font-style: italic;
}
.highlight .cp {
  color: #b2fd6d;
  font-weight: bold;
  font-style: italic;
}
.highlight .err {
  color: #fefeec;
  background-color: #cc0000;
}
.highlight .gr {
  color: #fefeec;
  background-color: #cc0000;
}
.highlight .k, .highlight .kd, .highlight .kv {
  color: #f6dd62;
  font-weight: bold;
}
.highlight .o, .highlight .ow {
  color: #4df4ff;
}
.highlight .p, .highlight .pi {
  color: #4df4ff;
}
.highlight .gd {
  color: #cc0000;
}
.highlight .gi {
  color: #b2fd6d;
}
.highlight .ge {
  font-style: italic;
}
.highlight .gs {
  font-weight: bold;
}
.highlight .gt {
  color: #dee5e7;
  background-color: #4e5d62;
}
.highlight .kc {
  color: #f696db;
  font-weight: bold;
}
.highlight .kn {
  color: #ffb000;
  font-weight: bold;
}
.highlight .kp {
  color: #ffb000;
  font-weight: bold;
}
.highlight .kr {
  color: #ffb000;
  font-weight: bold;
}
.highlight .gh {
  color: #ffb000;
  font-weight: bold;
}
.highlight .gu {
  color: #ffb000;
  font-weight: bold;
}
.highlight .kt {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .no {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .nc {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .nd {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .nn {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .bp {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .ne {
  color: #b2fd6d;
  font-weight: bold;
}
.highlight .nl {
  color: #ffb000;
  font-weight: bold;
}
.highlight .nt {
  color: #ffb000;
  font-weight: bold;
}
.highlight .m, .highlight .mf, .highlight .mh, .highlight .mi, .highlight .il, .highlight .mo, .highlight .mb, .highlight .mx {
  color: #f696db;
  font-weight: bold;
}
.highlight .ld {
  color: #f696db;
  font-weight: bold;
}
.highlight .ss {
  color: #f696db;
  font-weight: bold;
}
.highlight .s, .highlight .sb, .highlight .sd, .highlight .s2, .highlight .sh, .highlight .sx, .highlight .sr, .highlight .s1 {
  color: #fff0a6;
  font-weight: bold;
}
.highlight .se {
  color: #4df4ff;
  font-weight: bold;
}
.highlight .sc {
  color: #4df4ff;
  font-weight: bold;
}
.highlight .si {
  color: #4df4ff;
  font-weight: bold;
}
.highlight .nb {
  font-weight: bold;
}
.highlight .ni {
  color: #999999;
  font-weight: bold;
}
.highlight .w {
  color: #BBBBBB;
}
.highlight .nf {
  color: #a8e1fe;
}
.highlight .py {
  color: #a8e1fe;
}
.highlight .na {
  color: #a8e1fe;
}
.highlight .nv, .highlight .vc, .highlight .vg, .highlight .vi {
  color: #a8e1fe;
  font-weight: bold;
}
    </style>
    <link href="stylesheets/screen.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print.css" rel="stylesheet" media="print" />
      <script src="javascripts/all.js"></script>
  </head>

  <body class="index" data-languages="[]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar.png" alt="Navbar" />
      </span>
    </a>
    <div class="tocify-wrapper">
      <img src="images/logo.png" alt="Logo" />
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <div id="toc">
      </div>
        <ul class="toc-footer">
            <li><a href='https://github.com/kengz/openai_lab'>OpenAI Lab Github</a></li>
            <li><a href='https://github.com/openai/gym'>OpenAI Gym Github</a></li>
            <li><a href='https://github.com/fchollet/keras'>Keras Github</a></li>
            <li><a href='https://youtu.be/qBhLoeijgtA'>RL Tutorial video part 1/2</a></li>
            <li><a href='https://youtu.be/wNSlZJGdodE'>RL Tutorial video part 2/2</a></li>
        </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        <h1 id="openai-lab">OpenAI Lab </br> <a href="https://github.com/kengz/openai_lab"><img src="https://img.shields.io/github/release/kengz/openai_lab.svg" alt="GitHub release" /></a> <a href="https://circleci.com/gh/kengz/openai_lab"><img src="https://circleci.com/gh/kengz/openai_lab.svg?style=shield" alt="CircleCI" /></a> <a href="https://www.codacy.com/app/kengzwl/openai_lab?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_content=kengz/openai_lab&amp;amp;utm_campaign=Badge_Grade"><img src="https://api.codacy.com/project/badge/Grade/9e55f845b10b4b51b213620bfb98e4b3" alt="Codacy Badge" /></a> <a href="https://www.codacy.com/app/kengzwl/openai_lab?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=kengz/openai_lab&amp;utm_campaign=Badge_Coverage"><img src="https://api.codacy.com/project/badge/Coverage/9e55f845b10b4b51b213620bfb98e4b3" alt="Codacy Badge" /></a> <a href="https://github.com/kengz/openai_lab"><img src="https://img.shields.io/github/stars/kengz/openai_lab.svg?style=social&amp;label=Star" alt="GitHub stars" /></a> <a href="https://github.com/kengz/openai_lab"><img src="https://img.shields.io/github/forks/kengz/openai_lab.svg?style=social&amp;label=Fork" alt="GitHub forks" /></a></h1>

<p><em>An experimentation system for Reinforcement Learning using OpenAI Gym, Tensorflow, and Keras.</em></p>

<p><em>OpenAI Lab</em> is created to do Reinforcement Learning (RL) like science - <em>theorize, experiment</em>. It provides an easy interface to <a href="https://gym.openai.com/">OpenAI Gym</a> and <a href="https://keras.io/">Keras</a>, with an automated experiment and evaluation framework.</p>

<p>This is <a href="#motivations">motivated by the problems we faced in RL research</a>: <em>the difficulty of building upon other&rsquo;s work, the lack of rigor in comparisons of research results, and the inertia to high level vision.</em></p>

<p>The Lab aims to make RL research more efficient and to encourage experimentation, by doing three things:</p>

<ol>
<li>Handles the basic RL environment and algorithm setups.</li>
<li>Provides a standard, extensible platform with reusable components for developing deep reinforcement learning algorithms.</li>
<li>Provides a rigorous experimentation system with logs, plots and analytics for testing new RL algorithms. Experimental settings are stored in standardized format for reproducibility and comparisons.</li>
</ol>

<p>With OpenAI Lab, we could focus on researching the essential elements of reinforcement learning such as the algorithm, policy, memory, and parameter tuning. It allows us to build agents efficiently using existing components with the implementations from research ideas. We could then test the research hypotheses systematically by running experiments.</p>

<p><em>Ultimately, the Lab is a generalized framework for doing reinforcement learning, agnostic of OpenAI Gym and Keras. Pytorch-based implementations are on the roadmap, for example.</em></p>

<h3 id="implemented-algorithms">Implemented Algorithms</h3>

<p>See the <strong><a href="#fitness-matrix">Fitness Matrix</a></strong> for the results of these algorithms below in the OpenAI gym environments.</p>

<table><thead>
<tr>
<th style="text-align: left">algorithm</th>
<th style="text-align: left">implementation</th>
<th style="text-align: left">eval score (pending)</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1312.5602">DQN</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/dqn.py">DQN</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1509.06461">Double DQN</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/double_dqn.py">DoubleDQN</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1511.06581">Dueling DQN</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Sarsa</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/deep_sarsa.py">DeepSarsa</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Off-Policy Sarsa</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/offpol_sarsa.py">OffPolicySarsa</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1511.05952">PER (Prioritized Experience Replay)</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/memory/prioritized_exp_replay.py">PrioritizedExperienceReplay</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Cross-entropy_method">CEM (Cross Entropy Method)</a></td>
<td style="text-align: left">next</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="http://incompleteideas.net/sutton/williams-92.pdf">REINFORCE</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf">DPG (Deterministic Policy Gradient) off-policy actor-critic</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/actor_critic.py">ActorCritic</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1509.02971">DDPG (Deep-DPG) actor-critic with target networks</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/ddpg.py">DDPG</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/pdf/1602.01783.pdf">A3C (asynchronous advantage actor-critic)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Dyna</td>
<td style="text-align: left">next</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1502.05477">TRPO</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Q*(lambda)</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Retrace(lambda)</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1703.01988">Neural Episodic Control (NEC)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1612.00796">EWC (Elastic Weight Consolidation)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>

<h3 id="run-the-lab">Run the Lab</h3>

<p>Next, see <a href="#installation">Installation</a> and jump to <a href="#quickstart">Quickstart</a>.</p>

<div style="max-width: 100%"><img alt="Timelapse of OpenAI Lab" src="./images/lab_demo_dqn.gif" /></div>

<p><em>Timelapse of OpenAI Lab, solving CartPole-v0.</em></p>

          <h1 id="installation"><a name="installation"></a>Installation</h1>

<p><strong>1.</strong> <strong>clone the repo</strong></p>

<p><code class="prettyprint">git clone https://github.com/kengz/openai_lab.git</code></p>

<p><em>If you plan to commit code, fork this repo then clone it instead.</em></p>

<p><strong>2.</strong> <strong>install dependencies</strong></p>

<p>Run the following commands to install:</p>

<ul>
<li>the system dependencies depending on your OS</li>
<li>the project dependencies</li>
</ul>

<p><em>For quick repeated setup on remote servers, instead of these commands, run the equivalent setup script: <code class="prettyprint">./bin/setup</code></em></p>
<pre class="highlight shell"><code><span class="c"># cd into project directory</span>
<span class="nb">cd </span>openai_lab/

<span class="c">### MacOS System Dependencies</span>
<span class="c"># Homebrew</span>
ruby -e <span class="s2">"</span><span class="k">$(</span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install<span class="k">)</span><span class="s2">"</span>
<span class="c"># OpenAI Gym dependencies</span>
brew install cmake boost boost-python sdl2 swig wget
<span class="c"># noti</span>
<span class="o">(</span>curl -L https://github.com/variadico/noti/releases/download/v2.5.0/noti2.5.0.darwin-amd64.tar.gz | tar -xz<span class="o">)</span>; sudo mv noti /usr/local/bin/
<span class="c"># Node &gt;= v7.0</span>
brew install node
<span class="c"># Python &gt;= v3.0</span>
brew install python3

<span class="c">### Linux Ubuntu System Dependencies</span>
sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test <span class="o">&amp;&amp;</span> sudo apt-get update
sudo apt-get install -y gcc-4.9 g++-4.9 libhdf5-dev libopenblas-dev git python3-tk tk-dev
<span class="c"># OpenAI Gym dependencies</span>
sudo apt-get install -y cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig
<span class="c"># noti</span>
<span class="o">(</span>curl -L https://github.com/variadico/noti/releases/download/v2.5.0/noti2.5.0.linux-amd64.tar.gz | tar -xz<span class="o">)</span>; sudo mv noti /usr/local/bin/
<span class="c"># Node &gt;= v7.0</span>
<span class="o">(</span>curl -sL https://deb.nodesource.com/setup_7.x | sudo -E bash -<span class="o">)</span>; sudo apt-get install -y nodejs
<span class="c"># Python &gt;= v3.0</span>
sudo apt-get -y install python3-dev python3-pip python3-setuptools


<span class="c">### Project Dependencies</span>
./bin/copy-config
npm install; sudo npm i -g grunt-cli
<span class="c"># option 1: pip (ensure it is python3)</span>
pip3 install -r requirements.txt
<span class="c"># option 2: virtualenv</span>
virtualenv .env
<span class="nb">source</span> .env/bin/activate
pip3 install -r requirements.txt
<span class="c"># option 3: conda</span>
conda env create -f environment.yml
<span class="nb">source </span>activate openai_lab
</code></pre>
<p><strong>3.</strong> <strong>setup config files</strong></p>

<p>Run <code class="prettyprint">./bin/copy-config</code>. This will create the config files from template, needed for lab <a href="#usage">usage</a>:</p>

<ul>
<li><code class="prettyprint">config/default.json</code> for local development, used when <code class="prettyprint">grunt</code> is ran without a production flag.</li>
<li><code class="prettyprint">config/production.json</code> for production lab run when <code class="prettyprint">grunt -prod</code> is ran with the production flag <code class="prettyprint">-prod</code>.</li>
</ul>

<h2 id="quickstart"><a name="quickstart"></a>Quickstart</h2>

<p>The Lab comes with experiments with the best found solutions. Run your first below.</p>

<h3 id="single-trial">Single Trial</h3>

<p>Run the single best trial for an experiment using lab command: <code class="prettyprint">grunt -best</code></p>

<p>Alternatively, the plain python command invoked above is: <code class="prettyprint">python3 main.py -e quickstart_dqn</code></p>

<aside class="notice">
Remember to activate virtualenv/conda when using plain python commands.
</aside>

<p>Then check your <code class="prettyprint">./data/</code> folder for graphs and data files.</p>

<p>The grunt command is recommended as it&rsquo;s easier to schedule and run multiple experiments with. It sources from <code class="prettyprint">config/default.json</code>, which should now have <code class="prettyprint">quickstart_dqn</code>; more can be added.</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"data_sync_destination"</span><span class="p">:</span><span class="w"> </span><span class="s2">"~/Dropbox/openai_lab/data"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"NOTI_SLACK_DEST"</span><span class="p">:</span><span class="w"> </span><span class="s2">"#rl-monitor"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"NOTI_SLACK_TOK"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GET_SLACK_BOT_TOKEN_FROM_https://my.slack.com/services/new/bot"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"experiments"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"quickstart_dqn"</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>This trial is the best <a href="https://github.com/kengz/openai_lab/pull/73">found solution agent</a> of <code class="prettyprint">DQN</code> solving <code class="prettyprint">Cartpole-v0</code>. You should see the Lab running like so:</p>

<p><img src="images/lab_demo_dqn.gif" title="Timelapse of OpenAI Lab" alt="Lab demo dqn" /></p>

<h3 id="experiment-with-multiple-trials">Experiment with Multiple Trials</h3>

<p>Next step is to run a small experiment that searches for the best trial solutions.</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"quickstart_dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="err">...</span><span class="w">
    </span><span class="s2">"param_range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">64</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>This is under under <code class="prettyprint">quickstart_dqn</code> in <code class="prettyprint">rl/spec/classic_experiment_specs.json</code>. The experiment studies the effect of varying learning rate <code class="prettyprint">lr</code> and the DQN neural net architecture <code class="prettyprint">hidden_layers</code>. If you like, change the <code class="prettyprint">param_range</code> to try more values.</p>

<p>Then, run: <code class="prettyprint">grunt</code></p>

<p>Alternatively the plain python command is: <code class="prettyprint">python3 main.py -bp -e dqn</code></p>

<p>Then check your <code class="prettyprint">./data/</code> folder for graphs and data files.</p>

<p>The experiment will take about 15 minutes (depending on your machine). It will produce experiment data from the trials. Refer to <a href="#analysis">Analysis</a> on how to interpret them.</p>

<h3 id="next-up">Next Up</h3>

<p>We recommend:</p>

<ul>
<li>Continue reading below for the optional installation steps.</li>
<li><a href="#solutions">Solutions</a> to see some existing solutions to start your agent from, as well as find environments/high scores to beat.</li>
<li><a href="#agents">Agents</a> on how to create your agents from existing components, then add your own.</li>
<li><a href="#usage">Usage</a> to continue reading the doc.</li>
</ul>

<h2 id="updating-lab">Updating Lab</h2>

<p>Check the Lab&rsquo;s latest <a href="https://github.com/kengz/openai_lab/releases">release versions here</a>.</p>

<ul>
<li>If you cloned directly the Lab, update with <code class="prettyprint">git pull</code></li>
<li>If you forked, <a href="https://help.github.com/articles/configuring-a-remote-for-a-fork/">setup a remote</a> and <a href="https://help.github.com/articles/syncing-a-fork/">update fork</a></li>
</ul>
<pre class="highlight shell"><code><span class="c"># update direct clone</span>
git pull

<span class="c"># update fork</span>
git fetch upstream
git merge upstream/master
</code></pre>
<h2 id="setup-data-auto-sync">Setup Data Auto-sync</h2>

<p>We find it extremely useful to have data file-sync when running the lab on a remote server. This allows us to have a live view of the experiment graphs and data on our Dropbox app, on a computer or a smartphone.</p>

<p>For auto-syncing lab <code class="prettyprint">data/</code> we use <a href="http://gruntjs.com/">Grunt</a> file watcher for automatically copying data files to Dropbox. In your dropbox, set up a shared folder <code class="prettyprint">~/Dropbox/openai_lab/data</code> and sync to desktop.</p>

<p>Setup the config key <code class="prettyprint">data_sync_destination</code> in <code class="prettyprint">config/{default.json, production.json}</code>.</p>

<aside class="notice">
This step is optional; needed only when running production mode.
</aside>

<h2 id="setup-auto-notification">Setup Auto-notification</h2>

<p>Experiments take a while to run, and we find it useful also to be notified automatically on completion. We use <a href="https://github.com/variadico/noti">noti</a>, which is also installed with <code class="prettyprint">bin/setup</code>.</p>

<p>Set up a Slack, create a new channel <code class="prettyprint">#rl_monitor</code>, and get a <a href="https://my.slack.com/services/new/bot">Slack bot token</a>.</p>

<p>Setup the config keys <code class="prettyprint">NOTI_SLACK_DEST</code>, <code class="prettyprint">NOTI_SLACK_TOK</code> in <code class="prettyprint">config/{default.json, production.json}</code>.</p>

<aside class="notice">
This step is optional; useful when running production mode.
</aside>

<p><img src="images/noti.png" title="Notifications from the lab running on our remote server beast" alt="Noti" />
<em>Notifications from the lab running on our remote server beast.</em></p>

<h2 id="hardware">Hardware</h2>

<p>For setting up your own hardware, especially with a GPU, googling will help more than we could. Also, setup is usually non-trivial since there&rsquo;re so many moving parts. Here&rsquo;s the recommended references:</p>

<ul>
<li><a href="https://pcpartpicker.com/list/xdbWBP">A ~$1000 PC build</a> (more expensive now ~$1200; buy your parts during Black Friday/sales.)</li>
<li><a href="https://www.tensorflow.org/install/install_linux">The official TensorFlow installation guide, with GPU setup info</a></li>
<li><a href="http://christopher5106.github.io/nvidia/2016/12/30/commands-nvidia-install-ubuntu-16-04.html">Getting CUDA 8 to Work With openAI Gym on AWS and Compiling Tensorflow for CUDA 8 Compatibility</a></li>
<li><a href="https://github.com/openai/gym/issues/366">Major OpenAI issue with SSH with xvfb failing with NVIDIA Driver due to opengl files</a></li>
<li><a href="http://askubuntu.com/questions/149206/how-to-install-nvidia-run">NVIDIA cannot install due to X server running</a></li>
<li><a href="http://askubuntu.com/questions/759641/cant-get-nvidia-drivers-working-with-16-04-logs-out-right-after-login">When login fails on Ubuntu after Nvidia installation</a></li>
</ul>

          <h1 id="usage"><a name="usage"></a>Usage</h1>

<p><em>To understand the Lab&rsquo;s <a href="#structure">Framework and Demo, skip to the next section.</a></em></p>

<p>The general flow for running a production lab is:</p>

<ol>
<li>Specify experiment specs in <code class="prettyprint">rl/spec/*_experiment_specs.json</code>, e.g. <code class="prettyprint">&quot;dqn&quot;, &quot;lunar_dqn&quot;</code></li>
<li>Specify the names of the experiments to run in <code class="prettyprint">config/production.json</code></li>
<li>Run the lab, e.g. <code class="prettyprint">grunt -prod -resume</code></li>
</ol>

<p>Grunt will read off the JSON file in <code class="prettyprint">config/</code>, which looks like:</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"data_sync_destination"</span><span class="p">:</span><span class="w"> </span><span class="s2">"~/Dropbox/openai_lab/data"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"NOTI_SLACK_DEST"</span><span class="p">:</span><span class="w"> </span><span class="s2">"#rl-monitor"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"NOTI_SLACK_TOK"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GET_SLACK_BOT_TOKEN_FROM_https://my.slack.com/services/new/bot"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"experiments"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"dqn"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"lunar_dqn"</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<h2 id="commands">Commands</h2>

<p>We use <a href="http://gruntjs.com/">Grunt</a> to run the lab - set up experiments, pause/resume lab, run analyses, sync data, notify on completion. Internally <code class="prettyprint">grunt</code> runs the <code class="prettyprint">python</code> command (harder to use), logged to stdout as <code class="prettyprint">&gt;&gt; Composed command: ... python3 main.py -bp -t 5 -e dev_dqn | tee ./data/terminal.log;</code></p>

<p>The useful grunt commands are:</p>
<pre class="highlight shell"><code><span class="c"># when developing experiments specified in default.json</span>
grunt
<span class="c"># developing, run single best trial without param selection</span>
grunt -best

<span class="c"># run real lab experiments specified in production.json</span>
grunt -prod
<span class="c"># run lab over ssh on remote server</span>
grunt -prod -remote
<span class="c"># resume lab (previously incomplete experiments)</span>
grunt -prod -remote -resume


<span class="c"># clear data/ folder and cache files</span>
grunt clear
</code></pre>
<p>See below for the full <a href="#grunt-cmd">Grunt Command Reference</a> or the <a href="#python-cmd">Python Command Reference</a>.</p>

<p><strong>development</strong> mode:</p>

<ul>
<li>All grunt commands default to this mode</li>
<li>specify your dev experiment in <code class="prettyprint">config/default.json</code></li>
<li>use only when developing your new algorithms</li>
<li>the file-sync is in mock mode (emulated log without real file copying)</li>
<li>no auto-notification</li>
</ul>

<p><strong>production</strong> mode:</p>

<ul>
<li>append the flag <code class="prettyprint">-prod</code> to your <code class="prettyprint">grunt</code> command</li>
<li>specify your full experiments in <code class="prettyprint">config/production.json</code></li>
<li>use when running experiments for real</li>
<li>the file-sync is real</li>
<li>has auto-notification to Slack channel</li>
</ul>

<h2 id="run-remotely">Run Remotely</h2>

<p>If you&rsquo;re using a remote server, run the commands inside a <code class="prettyprint">screen</code>. That is, log in via ssh, start a screen, run, then detach screen.</p>
<pre class="highlight shell"><code><span class="c"># enter the screen with the name "lab"</span>
screen -S lab
<span class="c"># run real lab over ssh, in resume mode</span>
grunt -prod -remote -resume
<span class="c"># use Cmd+A+D to detach from screen, then Cmd+D to disconnect ssh</span>

<span class="c"># to resume screen next time</span>
screen -r lab
<span class="c"># use Cmd+D to terminate screen when lab ends</span>
</code></pre>
<p>Since a remote server is away, you should check the system status occasionally to ensure no overrunning processes (memory growth, large processes, overheating). Use <a href="https://github.com/nicolargo/glances"><code class="prettyprint">glances</code></a> (already installed in <code class="prettyprint">bin/setup</code>) to monitor your expensive machines.</p>

<aside class="notice">
To monitor your system (CPU, RAM, GPU), run <code>glances</code>
</aside>

<p><img src="images/glances.png" title="Glances to monitor your system" alt="Glances" />
<em>Glances on remote server beast.</em></p>

<h2 id="resume-lab">Resume Lab</h2>

<p>Experiments take a long time to complete, and if your process gets terminated, resuming the lab is trivial with a <code class="prettyprint">-resume</code> flag: <code class="prettyprint">grunt -prod -remote -resume</code>. This will use the <code class="prettyprint">config/history.json</code>:</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dqn-2017_03_19_004714"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>The <code class="prettyprint">config/history.json</code> is created in the last run that maps <code class="prettyprint">experiment_name</code>s to <code class="prettyprint">experiment_id</code>s, and resume any incomplete experiments based on that <code class="prettyprint">experiment_id</code>. You can manually tweak the file to set the resume target of course.</p>

<h2 id="grunt-command-reference"><a name="grunt-cmd"></a>Grunt Command Reference</h2>

<p>By default the <code class="prettyprint">grunt</code> command (no task or flag) runs the lab in <code class="prettyprint">development</code> mode using <code class="prettyprint">config/default.json</code>.</p>

<p>The basic grunt command pattern is</p>
<pre class="highlight shell"><code>grunt &lt;task&gt; -&lt;flag&gt;

<span class="c"># again, the useful grunt commands are:</span>

<span class="c"># when developing experiments specified in default.json</span>
grunt
<span class="c"># developing, run single best trial without param selection</span>
grunt -best

<span class="c"># run real lab experiments specified in production.json</span>
grunt -prod
<span class="c"># run lab over ssh on remote server</span>
grunt -prod -remote
<span class="c"># resume lab (previously incomplete experiments)</span>
grunt -prod -remote -resume

<span class="c"># clear data/ folder and cache files</span>
grunt clear
</code></pre>
<p>The <code class="prettyprint">&lt;task&gt;</code>s are:</p>

<ul>
<li><em>(default empty)</em>: run the lab</li>
<li><code class="prettyprint">clear</code>: clear the <code class="prettyprint">data/</code> folder and cache files. <strong>Be careful</strong> and make sure your data is already copied to the sync location</li>
</ul>

<p>The <code class="prettyprint">&lt;flag&gt;</code>s are:</p>

<ul>
<li><code class="prettyprint">-prod</code>: production mode, use <code class="prettyprint">config/production.json</code></li>
<li><code class="prettyprint">-resume</code>: resume incomplete experiments from <code class="prettyprint">config/history.json</code></li>
<li><code class="prettyprint">-remote</code>: when running over SSH, supplies this to use a fake display</li>
<li><code class="prettyprint">-best</code>: run the finalized experiments with gym rendering and live plotting; without param selection. This uses the default <code class="prettyprint">param</code> in <code class="prettyprint">experiment_specs.json</code> that shall be updated to the best found.</li>
<li><code class="prettyprint">-debug</code>: verbose debug logging. This is for lab-level development only.</li>
<li><code class="prettyprint">-quiet</code>: mute all python logging in grunt. This is for lab-level development only.</li>
</ul>

<h2 id="python-command-reference"><a name="python-cmd"></a>Python Command Reference</h2>

<p>The Python command is invoked inside <code class="prettyprint">Gruntfile.js</code> under the <code class="prettyprint">composeCommand</code> function. Change it if you need to.</p>

<aside class="notice">
Remember to activate virtualenv/conda when using plain python commands.
</aside>

<p>The basic python command pattern is:</p>
<pre class="highlight shell"><code>python3 main.py -&lt;flag&gt;

<span class="c"># most common example, with piping of terminal log</span>
python3 main.py -bp -t 5 -e dqn | tee -a ./data/terminal.log;
</code></pre>
<p>The python command <flag>s are:</p>

<ul>
<li><code class="prettyprint">-b</code>: blind mode, do not render graphics. Default: <code class="prettyprint">False</code></li>
<li><code class="prettyprint">-d</code>: log debug info. Default: <code class="prettyprint">False</code></li>
<li><code class="prettyprint">-e &lt;experiment&gt;</code>: specify which inside the <code class="prettyprint">rl/spec/*_experiment_spec.json</code> to run. Default: <code class="prettyprint">-e dev_dqn</code>. Can be a <code class="prettyprint">experiment_name, experiment_id</code>.</li>
<li><code class="prettyprint">-p</code>: run param selection. Default: <code class="prettyprint">False</code></li>
<li><code class="prettyprint">-q</code>: quiet mode, log warning only. Default: <code class="prettyprint">False</code></li>
<li><code class="prettyprint">-t &lt;times&gt;</code>: the number of sessions to run per trial. Default: <code class="prettyprint">1</code></li>
<li><code class="prettyprint">-x &lt;max_episodes&gt;</code>: Manually specifiy max number of episodes per trial. Default: <code class="prettyprint">-1</code> and program defaults to value in <code class="prettyprint">rl/spec/problems.json</code></li>
</ul>

          <h1 id="experiments"><a name="experiments"></a>Experiments</h1>

<p>The experimental framework design and terminology should be familiar, since they&rsquo;re borrowed from experimental science. The Lab runs experiments and produces data for <a href="#analysis">analysis</a>.</p>

<h2 id="definition">Definition</h2>

<p>An <strong>experiment</strong> runs separate <strong>trials</strong> by varying parameters. Each <strong>trial</strong> runs multiple <strong>sessions</strong> for averaging the results.</p>

<p>An experiment consists of:</p>

<ul>
<li>an <strong>environment</strong> (problem) from <a href="https://gym.openai.com/envs">OpenAI Gym</a></li>
<li>an <strong>agent</strong> to solve the environment.</li>
</ul>

<aside class="notice">
An experiment runs the variations of agent by changing its parameters (experiment variables) while holding others constants (control), and measure the fitness_score (outcome) to solve the environment.
</aside>

<h2 id="specification">Specification</h2>

<p>An experiment is specified by an <code class="prettyprint">experiment_spec</code> in <code class="prettyprint">rl/spec/*_experiment_specs.json</code>.</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GridSearch"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"AdamOptimizer"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"NoPreProcessor"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="s2">"param_range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.005</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02</span><span class="p">],</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.95</span><span class="p">,</span><span class="w"> </span><span class="mf">0.97</span><span class="p">,</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>It consists of:</p>

<ul>
<li><code class="prettyprint">experiment_name</code>: the key of the JSON. e.g. <code class="prettyprint">dqn</code></li>
<li><code class="prettyprint">problem</code>: name of the environment. e.g. <code class="prettyprint">CartPole-v0</code></li>
<li><strong>agent</strong>: and its components in <code class="prettyprint">rl/</code>, specified by the class name

<ul>
<li><code class="prettyprint">Agent</code> (Learning algorithm): decision function for learning from experiences gained by acting in an environment (eg Q-Learning, Sarsa). This is also the main class for agents. All other components of an agent are contained within this class.</li>
<li><code class="prettyprint">Policy</code>: decision function for acting in an environment. Controls exploration vs. exploitation trade off(e.g. epsilon greedy, boltzmann)</li>
<li><code class="prettyprint">Memory</code>: for storing experiences gained by acting in an environment. Controls how experiences are sampled for an agent to learn from. (e.g. random uniform with no forgetting, prioritized sampling with forgetting)</li>
<li><code class="prettyprint">Optimizer</code>: controls how to optimize the function approximators contained within the agent (e.g. Stochatic Gradient Descent, Adam) </li>
<li><code class="prettyprint">HyperOptimizer</code>: hyperparameter optimization algorithms used to vary the agent parameters and run trials with them (e.g grid search, random search)</li>
<li><code class="prettyprint">Preprocessor</code>: controls the transformations made to state representaions before being passed as inputs to the policy and learning algorithm. (e.g. no preprocessing, concatenating current and previous state). Useful for Atari.</li>
</ul></li>
<li><code class="prettyprint">param</code>: the default parameter values used (control variables)</li>
<li><code class="prettyprint">param_range</code>: the hyperparameter space ranges to search through by <code class="prettyprint">HyperOptimimzer</code> (experiment variables).</li>
</ul>

<h2 id="structure"><a name="structure"></a>Structure</h2>

<p>How <code class="prettyprint">experiments &gt; trials &gt; sessions</code> are organized and ran.</p>

<p>When the Lab runs an <strong>experiment</strong> with <code class="prettyprint">experiment_name</code> (e.g. <code class="prettyprint">dqn</code>):</p>

<ul>
<li>it creates a timestamped <code class="prettyprint">experiment_id</code> (<code class="prettyprint">dqn-2017_03_19_004714</code>)</li>
<li>the <strong>experiment</strong> runs multiple trials over the hyperparameter space

<ul>
<li>the trials are ordered for resumability (in case machine dies)</li>
<li>each trial has <code class="prettyprint">trial_id</code> (<code class="prettyprint">dqn-2017_03_19_004714_t0</code>), tied to a unique set of param values</li>
<li>a <strong>trial</strong> runs multiple sessions

<ul>
<li>each <strong>session</strong> has <code class="prettyprint">session_id</code> (<code class="prettyprint">dqn-2017_03_19_004714_t0_s0</code>)</li>
<li>a session runs the environment-agent, produces graphs and <code class="prettyprint">session_data</code> i.e. <code class="prettyprint">sys_vars</code></li>
<li>the session saves its graph to <code class="prettyprint">&lt;session_id&gt;.png</code></li>
<li>the session returns <code class="prettyprint">sys_vars</code> to its trial</li>
</ul></li>
<li>the trial gathers all the <code class="prettyprint">sys_vars</code>, run some averaging analytics, then compose all that into <code class="prettyprint">trial_data</code></li>
<li>the trial returns the <code class="prettyprint">trial_data</code> and saves it to <code class="prettyprint">&lt;trial_id&gt;.json</code></li>
</ul></li>
<li>the experiment composes all <code class="prettyprint">trial_data</code> into a <code class="prettyprint">experiment_data</code></li>
<li>it runs analytics to produce graphs <code class="prettyprint">&lt;experiment_id&gt;_analysis.png, &lt;experiment_id&gt;_correlation.png</code></li>
<li>it compute the <code class="prettyprint">fitness_score</code> for each trial, rank them by best-first, then save the data grid to <code class="prettyprint">&lt;experiment_id&gt;_analysis_data.csv</code></li>
<li>experiment ends</li>
</ul>

<h2 id="lab-demo"><a name="demo"></a>Lab Demo</h2>

<p>Given the framework explained above, here&rsquo;s a quick demo.</p>

<p>Suppose we aim to solve the CartPole-v0 problem with the plain DQN agent.  Suppose again for this experiment, we implement a new agent component, namely a <code class="prettyprint">Boltzmann</code> policy, and try to find the best parameter sets for this new agent.</p>

<h3 id="specify-experiment">Specify Experiment</h3>

<p>The example below is fully specified in <code class="prettyprint">rl/spec/classic_experiment_specs.json</code> under <code class="prettyprint">dqn</code>:</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GridSearch"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"AdamOptimizer"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"NoPreProcessor"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="s2">"param_range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.005</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02</span><span class="p">],</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.95</span><span class="p">,</span><span class="w"> </span><span class="mf">0.97</span><span class="p">,</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>Specifically of interests, we have specified the variables:</p>

<ul>
<li><em>experiment_name</em>: <code class="prettyprint">dqn</code></li>
<li><em>problem</em>: <a href="https://gym.openai.com/envs/CartPole-v0">CartPole-v0</a></li>
<li><em>variable agent component</em>: <code class="prettyprint">Boltzmann</code> policy</li>
<li><em>control agent variables</em>:

<ul>
<li><code class="prettyprint">DQN</code> agent</li>
<li><code class="prettyprint">LinearMemoryWithForgetting</code></li>
<li><code class="prettyprint">AdamOptimizer</code></li>
<li><code class="prettyprint">NoPreProcessor</code></li>
</ul></li>
<li><em>hyperparameter space</em>: the <code class="prettyprint">&quot;param_range&quot;</code> JSON</li>
<li><em>hyperparameter optimizer</em>: <code class="prettyprint">GridSearch</code></li>
</ul>

<p>Given <code class="prettyprint">GridSearch HyperOptimizer</code>, this <strong>experiment</strong> will try all the discrete combinations of the <code class="prettyprint">param_range</code>, which makes for <code class="prettyprint">4x4x5=80</code> trials. Each <strong>trial</strong> will run a max of 5 <strong>sessions</strong> (terminate on 2 if fail to solve). Overall, this experiments will run at most <code class="prettyprint">80 x 5 = 400</code> sessions, then produce <code class="prettyprint">experiment_data</code> and the analytics.</p>

<h3 id="lab-workflow">Lab Workflow</h3>

<p>The example workflow to setup this experiment is as follow:</p>

<ol>
<li>Add the new theorized component <code class="prettyprint">Boltzmann</code> in <code class="prettyprint">rl/policy/boltzmann.py</code></li>
<li>Specify <code class="prettyprint">dqn</code> experiment spec in <code class="prettyprint">rl/spec/classic_experiment_spec.json</code> to include this new variable, reuse the other existing RL components, and specify the param range.</li>
<li>Add this experiment to the lab queue in <code class="prettyprint">config/production.json</code></li>
<li>Run experiment with <code class="prettyprint">grunt -prod</code></li>
<li>Analyze the graphs and data</li>
</ol>

<p>Now that you can produce the experiment data and graphs, see how to <a href="#analysis">analyze them</a>.</p>

          <h1 id="analysis"><a name="analysis"></a>Analysis</h1>

<p>Once the Lab is running experiments, it will produce data. This section details how to analyze and understand the data, before we can contribute to the <a href="#solutions">Solutions</a>.</p>

<p>An experiment produces 3 types of data files in the folder <code class="prettyprint">data/&lt;experiment_id&gt;/</code>:</p>

<ul>
<li><strong>session plots</strong>: <code class="prettyprint">&lt;session_id&gt;.png</code></li>
<li><strong>trial_data</strong>: <code class="prettyprint">&lt;trial_id&gt;.json</code></li>
<li><strong>experiment data</strong>:

<ul>
<li><code class="prettyprint">&lt;experiment_id&gt;_analysis_data.csv</code></li>
<li><code class="prettyprint">&lt;experiment_id&gt;_analysis.png</code></li>
<li><code class="prettyprint">&lt;experiment_id&gt;_analysis_correlation.png</code></li>
</ul></li>
</ul>

<aside class="notice">
Refer to <a href="#structure">Experiments > Structure</a> to see how the files are produced.
</aside>

<p>We will illustrate with an example experiment from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn solution PR</a>.</p>

<h2 id="session-graphs">Session Graphs</h2>

<blockquote>
<p><img alt="The best session graph" src="https://cloud.githubusercontent.com/assets/8209263/24180935/404370ea-0e8e-11e7-8f20-f8691ee03e7b.png" />
<em>The best session graph from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn-2017_03_19_004714</a> experiment. From the session graph we can see that the agent starts learning the CartPole-v0 task at around episode 15, then solves it before episode 20. Over time the loss decreases, the solution becomes stabler, and the mean rewards increases until the session is solved reliably.</em></p>
</blockquote>

<p>When an experiment is running, the lab will plot the session graphs live, one for each session.</p>

<h3 id="how-to-read">How to read</h3>

<p>A session graph has 3 subplots:</p>

<ol>
<li><p><strong>total rewards and exploration rate vs episode</strong>: directly records the (blue) total rewards attained at the end of each episode, in relation to the (red) exploration rate (<code class="prettyprint">epsilon</code>, <code class="prettyprint">tau</code>, etc. depending on the policy).</p>

<p>The 2 lines usually show negative correlation - when the exploration rate drops, the total rewards should rise. When a solution becomes stable, the blue line should stay around its max.</p></li>
<li><p><strong>mean rewards over the last 100 episodes vs episode</strong>: measures the 100-episode mean of the total rewards from above.</p>

<p>Defined by OpenAI, this metric is usually how a solution is identified - when it hits a target solution score, which would mean that the solution is sufficiently <em>strong</em> and <em>stable</em>.</p></li>
<li><p><strong>loss vs time</strong>: measures the loss of the agent&rsquo;s neural net. This graph is all the losses  concatenated over time, over all episodes.</p>

<p>There is no specific unit for the loss as it depends on what loss function is used in the NN architecture (typically <code class="prettyprint">mean_squared_error</code>). As the NN starts getting more accurate, the loss should decrease.</p></li>
</ol>

<aside class="notice">
When developing a new algorithm, use the session graph to immediately see how the agent is performing without needing to wait for the entire session to complete.
</aside>

<h2 id="analysis-graph">Analysis Graph</h2>

<blockquote>
<p><img src="https://cloud.githubusercontent.com/assets/8209263/24582680/bc9fd9ae-1702-11e7-881b-38f7f379d6ff.png" title="Analysis graph" alt="Bc9fd9ae 1702 11e7 881b 38f7f379d6ff" />
<em>The analysis graph from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn-2017_03_19_004714</a> experiment. There&rsquo;re numerous dark points with solved_ratio 1, which is expected since CartPole-v0 is the simplest environment. There are clear trends cross the x-values - gamma=0.95 is unstable; 2-hidden-layer NN is unsuitable for the problem, but wider 1-hidden-layer is good; learning rate lr=0.001 is stabler, but lr=0.02 is a good balance between stability and fitness_score.</em></p>
</blockquote>

<p>The <strong>analysis graph</strong> is the primary graph used to judge the overall experiment - how all the trials perform. It is a pair-plot of the <em>measurement metrics on the y-axis</em>, and the <em>experiment variables on the x-axis</em>.</p>

<h3 id="the-y-axis-measurement-metrics">The y-axis measurement metrics</h3>

<ol>
<li><p><code class="prettyprint">fitness_score</code>: the final evaluation metric the Lab uses to select a fit agent (an agent with the fit parameter set for that class of Agent). The design and purpose of it is more involved - see <a href="#metrics">metrics</a> for more.</p></li>
<li><p><code class="prettyprint">mean_rewards_stats_max</code>: the max of all the <code class="prettyprint">mean_rewards</code> over all the sessions of a trial. Measures the max solution power of a trial.</p></li>
<li><p><code class="prettyprint">max_total_rewards_stats_mean</code>: the statistical mean of all the <code class="prettyprint">max_total_rewards</code> over all the sessions of a trial. Measures the agent&rsquo;s average peak performance.</p></li>
<li><p><code class="prettyprint">epi_stats_min</code>: the min of the termination episode of a session, i.e. the fastest solved session of a trial. The lower the better, as it would imply that the agent can solve the environment faster.</p></li>
</ol>

<h3 id="the-hue-metrics"><a name="hue"></a>The hue metrics</h3>

<p>Each data point represents a trial, with the data averaged over its sessions. The points are colored (see legend) with the hue:</p>

<ul>
<li><code class="prettyprint">solved_ratio_of_sessions</code>: how many sessions are solved out of the total sessions in a trial, 0 means none, 1 means all.</li>
</ul>

<p>The granularity of the <code class="prettyprint">solved_ratio_of_sessions</code> depends on the number of sessions ran per trial. From experience, we settle on 5 sessions per trial as it&rsquo;s the best tradeoff between granularity and computation time.</p>

<p>Multiple sessions allow us to observe the consistency of an agent. As we have noticed across the parameter space, there is a spectrum of solvability: agents who cannot solve at all, can solve occasionally, and can always solve. The agents that solves occasionally can be valuable when developing an new algorithm, and most people will throw them away - this is bad when a strong agent is hard to find in the early stage.</p>

<h3 id="how-to-read">How to read</h3>

<p>Every subplot in the graph shows the distribution of all the trial points in the pair of <em>y vs x</em> variables, with the other <em>x&rsquo;</em> dimensions flattened. For each, observe the population distribution, y-positions, and trend across the x-axis.</p>

<p>Note that these will use <a href="http://seaborn.pydata.org/generated/seaborn.swarmplot.html">swarmplot</a> which allows us to see the distribution of points by spreading them horizontally to prevent overlap. However, when the x-axis has too many values (.e.g continuous x-values in random search), it will switch to scatter plot instead.</p>

<blockquote>
<p><img src="https://cloud.githubusercontent.com/assets/8209263/24582554/1fbefc98-1700-11e7-85f8-b0740b25007f.png" title="The acrobot random search analysis graph" alt="1fbefc98 1700 11e7 85f8 b0740b25007f" />
<em>An example from rand_acrobot with scatterplot instead of swarmplot for gamma and lr. This is when RandomSearch is used. The example is from the <a href="https://github.com/kengz/openai_lab/pull/94">rand_acrobot-2017_03_28_082754 PR</a></em></p>
</blockquote>

<p><strong>Population distribution</strong>: more darker points implies that the many trials could solve the environment consistently. Higher ratio of dark points also means the environment is easier for the agent. If the points are closer and the distribution has smaller vertical gaps, then the <em>x</em> is a stabler value for the <em>y</em> value even when other <em>x&rsquo;</em> dimensions vary. In a scatterplot, clustering of points in a random search also shows the convergence of the search.</p>

<p><strong>trend across y-values</strong>: the fitter trial will show up higher in the y-axes (except for <code class="prettyprint">epi_stats_min</code>). Generally good solutions are scarce and they show up at higher <code class="prettyprint">fitness_score</code>, whereas the non-solutions get clustered in the lower region. Notice how the <code class="prettyprint">fitness_score</code> plots can clearly distinguish the good solutions (darker points), whereas in the <code class="prettyprint">mean_rewards_stats_max</code> and <code class="prettyprint">max_total_rewards_stats_mean</code> plots it is hard to tell apart. We will discuss how the custom-formulated <code class="prettyprint">fitness_score</code> function achieves this in the <a href="#metrics">metrics</a> section.</p>

<p><strong>trend across x-values</strong>: to find a stable and good <em>x-value</em>, observe the vertical gaps in distribution, the clustering of darker points. Usually there&rsquo;s one maxima with a steady trend towards it. Recall that the plots flatten the other <em>x&rsquo;</em> values, but the dependence on <em>x</em> value is usually very consistent across <em>x&rsquo;</em> that there will still be a flattened trend.</p>

<h2 id="correlation-graph">Correlation Graph</h2>

<blockquote>
<p><img src="https://cloud.githubusercontent.com/assets/8209263/24582681/be76f4ec-1702-11e7-9935-2491189ab1e6.png" title="Correlation graph" alt="Be76f4ec 1702 11e7 9935 2491189ab1e6" />
<em>The correlation graph from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn-2017_03_19_004714</a> experiment. We can see smooth contours of spectrum in them, suggesting that the x-values are stable - small change in values will not be catastrophic. There are 2 darker regions in the contour; the distribution confirms that gamma=0.999 and lower lr are indeed stabler, since they have higher populations of darker points. The instability of gamma=0.95 shows clearly as 2 segments of stacked bar with huge contrast.</em></p>
</blockquote>

<p>The <strong>correlation graph</strong> reveals pairwise x-value correlations that is flattened in the analysis graph. This is a pair-plot between the orderable parameter variables.</p>

<h3 id="how-to-read">How to read</h3>

<p>The diagonals simply shows the population distribution for that x-value; the off-diagonal plots show the fitness score heatmap that tells how to best combine separate parameter values. Note that the heatmap color scheme does not represent absolute fitness, but points are colored by which <code class="prettyprint">fitness_score_bin</code> they fall into.</p>

<p>The points are semi-transparent, so if they overlap, their colors will stack instead of hiding the points behind.</p>

<h2 id="data">Data</h2>

<p>After glancing through the graphs, it will be easier to understand the data and find the targets.</p>

<h3 id="how-to-read">How to read</h3>

<p>The <code class="prettyprint">&lt;experiment_id&gt;_analysis_data.csv</code> will show the data for each trial, sorted by the highest <code class="prettyprint">fitness_score</code> first. The left columns are the measured output values; then they&rsquo;re separated by the <code class="prettyprint">trial_id</code>; the right columns are the parameter values for the trial.</p>

<p>The <code class="prettyprint">trial_id</code> will tell us which <code class="prettyprint">trial_data</code> to check for even more details on the best trials. Usually we can also spot some trend in the right parameter columns.</p>

<p>The best <code class="prettyprint">&lt;trial_id&gt;.json</code> will show us directly what is its <code class="prettyprint">experiment_spec</code>, and more stats about the trial. When <a href="#solutions">submitting a solution PR</a>, retrieve the <code class="prettyprint">experiment_spec</code> to update the default <code class="prettyprint">*_experiment_spec.json</code>, and get the <code class="prettyprint">fitness_score</code> from here too.</p>

<table><thead>
<tr>
<th style="text-align: left">best_session_epi</th>
<th style="text-align: left">best_session_id</th>
<th style="text-align: left">best_session_mean_rewards</th>
<th style="text-align: left">best_session_stability</th>
<th style="text-align: left">fitness_score</th>
<th style="text-align: left">mean_rewards_per_epi_stats_mean</th>
<th style="text-align: left">mean_rewards_stats_mean</th>
<th style="text-align: left">mean_rewards_stats_max</th>
<th style="text-align: left">epi_stats_mean</th>
<th style="text-align: left">epi_stats_min</th>
<th style="text-align: left">solved_ratio_of_sessions</th>
<th style="text-align: left">max_total_rewards_stats_mean</th>
<th style="text-align: left">trial_id</th>
<th style="text-align: left">variable_gamma</th>
<th style="text-align: left">variable_hidden_layers</th>
<th style="text-align: left">variable_lr</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">114</td>
<td style="text-align: left">dqn-2017_03_19_004714_t79_s2</td>
<td style="text-align: left">195.59</td>
<td style="text-align: left">0.845361</td>
<td style="text-align: left">9.635032</td>
<td style="text-align: left">1.326499</td>
<td style="text-align: left">195.404</td>
<td style="text-align: left">195.66</td>
<td style="text-align: left">154.2</td>
<td style="text-align: left">114.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t79</td>
<td style="text-align: left">0.999</td>
<td style="text-align: left">[64]</td>
<td style="text-align: left">0.02</td>
</tr>
<tr>
<td style="text-align: left">157</td>
<td style="text-align: left">dqn-2017_03_19_004714_t36_s4</td>
<td style="text-align: left">196.07</td>
<td style="text-align: left">1.010526</td>
<td style="text-align: left">9.282276</td>
<td style="text-align: left">1.155354</td>
<td style="text-align: left">195.602</td>
<td style="text-align: left">196.07</td>
<td style="text-align: left">169.0</td>
<td style="text-align: left">157.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t36</td>
<td style="text-align: left">0.97</td>
<td style="text-align: left">[64]</td>
<td style="text-align: left">0.001</td>
</tr>
<tr>
<td style="text-align: left">140</td>
<td style="text-align: left">dqn-2017_03_19_004714_t28_s0</td>
<td style="text-align: left">196.11</td>
<td style="text-align: left">0.989474</td>
<td style="text-align: left">9.178363</td>
<td style="text-align: left">1.179311</td>
<td style="text-align: left">195.564</td>
<td style="text-align: left">196.11</td>
<td style="text-align: left">167.2</td>
<td style="text-align: left">140.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t28</td>
<td style="text-align: left">0.97</td>
<td style="text-align: left">[32]</td>
<td style="text-align: left">0.001</td>
</tr>
<tr>
<td style="text-align: left">123</td>
<td style="text-align: left">dqn-2017_03_19_004714_t50_s0</td>
<td style="text-align: left">195.16</td>
<td style="text-align: left">0.96875</td>
<td style="text-align: left">9.074483</td>
<td style="text-align: left">1.276302</td>
<td style="text-align: left">195.136</td>
<td style="text-align: left">195.3</td>
<td style="text-align: left">160.6</td>
<td style="text-align: left">123.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t50</td>
<td style="text-align: left">0.99</td>
<td style="text-align: left">[32]</td>
<td style="text-align: left">0.01</td>
</tr>
<tr>
<td style="text-align: left">153</td>
<td style="text-align: left">dqn-2017_03_19_004714_t16_s2</td>
<td style="text-align: left">195.48</td>
<td style="text-align: left">1.010526</td>
<td style="text-align: left">8.98669</td>
<td style="text-align: left">1.159941</td>
<td style="text-align: left">195.466</td>
<td style="text-align: left">195.7</td>
<td style="text-align: left">168.6</td>
<td style="text-align: left">153.0</td>
<td style="text-align: left">1.0</td>
<td style="text-align: left">200.0</td>
<td style="text-align: left">dqn-2017_03_19_004714_t16</td>
<td style="text-align: left">0.95</td>
<td style="text-align: left">[64]</td>
<td style="text-align: left">0.001</td>
</tr>
</tbody></table>

<p><em><code class="prettyprint">dqn-2017_03_19_004714_analysis_data.csv</code>, top 5 trials, from the <a href="https://github.com/kengz/openai_lab/pull/73">dqn-2017_03_19_004714</a> experiment. We can see that among the dominating parameter values are gamma=0.999, hidden_layers=[64], lr=[0.02]. The best trial json below.</em></p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"experiment_spec"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GridSearch"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"AdamOptimizer"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"NoPreProcessor"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"experiment_name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dqn"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"decay"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
      </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.999</span><span class="p">,</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="s2">"metrics"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"best_session_epi"</span><span class="p">:</span><span class="w"> </span><span class="mi">114</span><span class="p">,</span><span class="w">
    </span><span class="s2">"best_session_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"dqn-2017_03_19_004714_t79_s2"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"best_session_mean_rewards"</span><span class="p">:</span><span class="w"> </span><span class="mf">195.59</span><span class="p">,</span><span class="w">
    </span><span class="s2">"best_session_stability"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.8453608</span><span class="p">,</span><span class="w">
    </span><span class="s2">"epi_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mf">154.2</span><span class="p">,</span><span class="w">
    </span><span class="s2">"fitness_score"</span><span class="p">:</span><span class="w"> </span><span class="mf">9.635032</span><span class="p">,</span><span class="w">
    </span><span class="s2">"max_total_rewards_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mi">200</span><span class="p">,</span><span class="w">
    </span><span class="s2">"mean_rewards_per_epi_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.326499</span><span class="p">,</span><span class="w">
    </span><span class="s2">"mean_rewards_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mf">195.404</span><span class="p">,</span><span class="w">
    </span><span class="s2">"solved_ratio_of_sessions"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="s2">"t_stats_mean"</span><span class="p">:</span><span class="w"> </span><span class="mi">199</span><span class="p">,</span><span class="w">
    </span><span class="s2">"time_taken"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0:41:19"</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="err">...</span><span class="w">
</span></code></pre>
<p>This concludes the analysis. See the <a href="https://github.com/kengz/openai_lab/pull/73">solution PR here</a>. The best trial is <code class="prettyprint">dqn-2017_03_19_004714_t79</code>, with <code class="prettyprint">fitness_score=9.635032</code>, and the variables:</p>

<ul>
<li><em>lr</em>: 0.02</li>
<li><em>gamma</em>: 0.999</li>
<li><em>hidden_layers_shape</em>: [64]</li>
</ul>

<p>Now that you know how to analyze the data,</p>

<ul>
<li><a href="#solutions">start finding problems to beat and submitting your solutions</a></li>
<li><a href="#metrics">learn more about the evaluation metrics</a></li>
</ul>

          <h1 id="solutions"><a name="solutions"></a>Solutions</h1>

<p>Agents and best solutions by OpenAI Lab users. We want people to start from working solutions instead of stumbling their ways there.</p>

<h2 id="submission-instructions">Submission instructions</h2>

<p>If you invent a new algorithm/combination that beats the best solutions, please submit a <a href="https://github.com/kengz/openai_lab/pulls">Pull Request</a> to OpenAI Lab. Refer to the <a href="https://github.com/kengz/openai_lab/blob/master/.github/PULL_REQUEST_TEMPLATE.md">PR template</a> for the submission guideline. See examples from the accepted <a href="https://github.com/kengz/openai_lab/pulls?q=is%3Apr+label%3Asolution+is%3Aclosed">solution PRs</a>.</p>

<p>To learn how to analyze experiment data, refer to <a href="#analysis">Analysis</a>.</p>

<h2 id="fitness-matrix"><a name="fitness-matrix"></a>Fitness Matrix</h2>

<p>A matrix of the best <code class="prettyprint">fitness_score</code> of <strong>Agents</strong> v.s. <strong>Environments</strong>, sourced from the accepted <a href="https://github.com/kengz/openai_lab/pulls?q=is%3Apr+label%3Asolution+is%3Aclosed">solution PRs</a>. See <a href="#metrics">Metric</a> for the design of fitness score and generalized metrics.</p>

<table><thead>
<tr>
<th style="text-align: left"></th>
<th style="text-align: left">DQN</th>
<th style="text-align: left">DoubleDQN</th>
<th style="text-align: left">Sarsa</th>
<th style="text-align: left">OffPolicySarsa</th>
<th style="text-align: left">DoubleDQN-PER</th>
<th style="text-align: left">ActorCritic</th>
<th style="text-align: left">DPG</th>
<th style="text-align: left">DDPG</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><strong>CartPole-v0</strong></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/73">9.635032</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/78">10.34826</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/91">12.98525</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/123">13.90989</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>CartPole-v1</strong></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/80">13.22935</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/82">16.06697</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/107">18.91624</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/124">30.57067</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Acrobot-v1</strong></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/94">-0.1051617</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/98">-0.1045992</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/97">-0.1127294</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>MountainCar-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/117">-0.03744196</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>MountainCarContinuous-v0</strong></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Pendulum-v0</strong></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/125">-0.2984732</a></td>
</tr>
<tr>
<td style="text-align: left"><strong>LunarLander-v2</strong></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/84">2.786624</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/87">2.992104</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/96">3.313421</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>LunarLanderContinuous-v2</strong></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left"><em>n/a</em></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>BipedalWalker-v2</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>BipedalWalkerHardcore-v2</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>CarRacing-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>AirRaid-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Alien-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Assault-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Breakout-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>MsPacman-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Pong-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Qbert-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>SpaceInvader-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>FlappyBird-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><strong>Snake-v0</strong></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>

<h2 id="agents-fitness-matrix"><a name="agents-matrix"></a>Agents Fitness Matrix</h2>

<p>A projection of the Fitness Matrix along the Agents axis. This shows overall status of the Agents in OpenAI Lab. Feel free to invent new ones! For more details, see <a href="#algorithms">Algorithms</a> and <a href="#families">Families of RL Algorithms</a>.</p>

<p><em>Pending: we have a generic formalization to cross-evaluate Agents using heatmap statistics; see <a href="#metrics">Metrics</a>. This is on the roadmap.</em></p>

<table><thead>
<tr>
<th style="text-align: left">algorithm</th>
<th style="text-align: left">implementation</th>
<th style="text-align: left">eval score (pending)</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1312.5602">DQN</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/dqn.py">DQN</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1509.06461">Double DQN</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/double_dqn.py">DoubleDQN</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1511.06581">Dueling DQN</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Sarsa</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/deep_sarsa.py">DeepSarsa</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Off-Policy Sarsa</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/offpol_sarsa.py">OffPolicySarsa</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1511.05952">PER (Prioritized Experience Replay)</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/memory/prioritized_exp_replay.py">PrioritizedExperienceReplay</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Cross-entropy_method">CEM (Cross Entropy Method)</a></td>
<td style="text-align: left">next</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="http://incompleteideas.net/sutton/williams-92.pdf">REINFORCE</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf">DPG (Deterministic Policy Gradient) off-policy actor-critic</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/actor_critic.py">ActorCritic</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1509.02971">DDPG (Deep-DPG) actor-critic with target networks</a></td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/blob/master/rl/agent/ddpg.py">DDPG</a></td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/pdf/1602.01783.pdf">A3C (asynchronous advantage actor-critic)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Dyna</td>
<td style="text-align: left">next</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1502.05477">TRPO</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Q*(lambda)</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Retrace(lambda)</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1703.01988">Neural Episodic Control (NEC)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left"><a href="https://arxiv.org/abs/1612.00796">EWC (Elastic Weight Consolidation)</a></td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>

<h2 id="environments-fitness-matrix"><a name="environments-matrix"></a>Environments Fitness Matrix</h2>

<p>A projection of the Fitness Matrix along the Environments axis. This shows the best solutions for the environments. The list of accepted solutions can be seen in the <a href="https://github.com/kengz/openai_lab/pulls?q=is%3Apr+label%3Asolution+is%3Aclosed">solution PRs</a>.</p>

<h3 id="classic-environments">Classic Environments</h3>

<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">CartPole-v0</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/123">13.90989</a></td>
<td style="text-align: left">3</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">cartpole_ac_softmax</td>
</tr>
<tr>
<td style="text-align: left">CartPole-v1</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/124">30.57067</a></td>
<td style="text-align: left">3</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">cartpole_v1_ac_softmax</td>
</tr>
<tr>
<td style="text-align: left">Acrobot-v1</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/98">-0.1045992</a></td>
<td style="text-align: left">-104.34</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">acrobot_offpol_sarsa</td>
</tr>
<tr>
<td style="text-align: left">MountainCar-v0</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/117">-0.03744196</a></td>
<td style="text-align: left">970</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">mountain_double_dqn</td>
</tr>
<tr>
<td style="text-align: left">MountainCarContinuous-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Pendulum-v0</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/125">-0.2984732</a></td>
<td style="text-align: left">-150.8091</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">pendulum_ddpg</td>
</tr>
</tbody></table>

<h3 id="box2d-environments">Box2D Environments</h3>

<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">LunarLander-v2</td>
<td style="text-align: left"><a href="https://github.com/kengz/openai_lab/pull/96">3.313421</a></td>
<td style="text-align: left">200</td>
<td style="text-align: left">kengz/lgraesser</td>
<td style="text-align: left">lunar_offpol_sarsa</td>
</tr>
<tr>
<td style="text-align: left">LunarLanderContinuous-v2</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">BipedalWalker-v2</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">BipedalWalkerHardcore-v2</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">CarRacing-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>

<h3 id="atari-environments">Atari Environments</h3>

<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">AirRaid-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Alien-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Assault-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Breakout-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">MsPacman-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Pong-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Qbert-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">SpaceInvader-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>

<h3 id="pygame-environments">PyGame Environments</h3>

<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">FlappyBird-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
<tr>
<td style="text-align: left">Snake-v0</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>

<h3 id="universe-environments">Universe Environments</h3>

<table><thead>
<tr>
<th style="text-align: left">problem</th>
<th style="text-align: left">fitness score</th>
<th style="text-align: left">epis before solve / best 100-epi mean</th>
<th style="text-align: left">author</th>
<th style="text-align: left">experiment_spec</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
<td style="text-align: left">-</td>
</tr>
</tbody></table>

          <h1 id="metrics"><a name="metrics"></a>Metrics</h1>

<p>The Lab setup allows us to run experiments at scale; the standardized framework also allows us to reliably compare multiple agents (algorithms) and environments (problems). These are shown with the <a href="#fitness-matrix">Fitness Matrix</a>, which also necessitates a higher level evaluation metric.</p>

<p>With the Lab, we are breeding multiple agents across many environments and selecting the best ones. Naturally, this selection metric is called the <code class="prettyprint">fitness_score</code>. Some evolutionary search algorithm for the <code class="prettyprint">HyperOptimizer</code> is on our <a href="#roadmap">roadmap</a>.</p>

<p>The Fitness Matrix is a projection from the parameter space of each agent-environment pair, where each matrix cell is the highest fitness score the agent could achieve in the environment.</p>

<p>To understand the bigger picture, the domain for the fitness function for each matrix cell is the parameter space of the agent conditioned on the environment. Inside the parameter space, each point gets mapped to a fitness score.</p>

<p>To analogize, see fitness score as temperature, then we have a heatmap inside the parameter space, and we are searching for the hottest point and recording that in a cell of the Fitness Matrix.</p>

<p>In this section, we will formalize these ideas.</p>

<h2 id="fitness-score"><a name="fitness"></a>Fitness Score</h2>

<p>The fitness function <code class="prettyprint">f</code> is the base function behind the Fitness Matrix and the fitness heatmap. It computes the fitness score for each point (trial) in a parameter space.</p>

<p>The fitness function&rsquo;s design is motivated by the need to have a richer evaluation of an agent, and the ability of the Lab to provide such data. We felt that the <code class="prettyprint">mean rewards</code> metric used in the OpenAI gym evaluation is insufficient, but it is included in our design under <em>strength</em>.</p>

<p>The fitness score of a trial is as follows:</p>

<p><code class="prettyprint">fitness_score = mean_rewards_per_epi_mean * [(1+stability_mean) * ((1+consistency)**2)] ** sign(mean_rewards_per_epi_mean)</code></p>

<p>or renaming variables by what the terms represent:</p>

<p><code class="prettyprint">fitness_score = power * distinguisher</code></p>

<p>where</p>

<ul>
<li><code class="prettyprint">power = mean_rewards_per_epi_mean</code></li>
<li><code class="prettyprint">distinguisher = amplifier ** sign(power)</code></li>
<li><code class="prettyprint">amplifier = (1+stability_mean) * ((1+consistency)**2)</code></li>
</ul>

<p>The fitness score is designed to capture the following:</p>

<ol>
<li><strong>strength</strong>: <code class="prettyprint">mean_rewards</code> (over the past 100 episodes)</li>
<li><strong>speed</strong>: <code class="prettyprint">1/epi</code></li>
<li><strong>stability</strong>: session-level stability, <code class="prettyprint">stable_epi_count / mastery_gap</code></li>
<li><strong>consistency</strong>: AKA trial-level stability, <code class="prettyprint">solved_ratio_of_session</code></li>
<li><strong>granularity</strong>: in <code class="prettyprint">1+stability</code> and <code class="prettyprint">1+consistency</code> to ensure partial solution doesn&rsquo;t get lost when stability and consistency = 0</li>
<li><strong>amplification</strong>: amplify by session-level stability linearly with <code class="prettyprint">*(1+stability)</code>, and by trial-level stability quadratically with <code class="prettyprint">*(1+consistency)**2</code></li>
<li><strong>distinguishability</strong>: multiply amplifier if <code class="prettyprint">power</code> is positive, else divide; essentially <code class="prettyprint">*(amplifier**sign(power))</code></li>
</ol>

<h3 id="strength">Strength</h3>

<p>The higher (more positive) the <code class="prettyprint">mean_rewards</code> an agent gets, the stronger it is. The mean is taken over the past 100 episodes, as common to OpenAI gym.</p>

<h3 id="speed">Speed</h3>

<p>Given two same <code class="prettyprint">mean_rewards</code>, the agent that achieves it in less episodes is faster, with <code class="prettyprint">speed = 1/epi</code>. This yields the notion of <code class="prettyprint">power = strength * speed = mean_rewards_per_epi</code>. Use the sessions-mean of a trial, i.e. <code class="prettyprint">mean_rewards_per_epi_mean</code> = mean of multiple <code class="prettyprint">mean_rewards_per_epi</code> values.</p>

<h3 id="stability">Stability</h3>

<p><code class="prettyprint">stability = stable_epi_count / mastery_gap</code> for a session, with range <code class="prettyprint">0.0 - 1.0</code> from unstable to stable. Measures session-level stability. Use the sessions-mean of a trial, i.e. mean of multiple <code class="prettyprint">stability</code> values.</p>

<ul>
<li><code class="prettyprint">stable_epi_count</code> = the number of stable episodes, where stable means an episode passes the <code class="prettyprint">r_threshold</code>.</li>
<li><code class="prettyprint">r_threshold</code> = <code class="prettyprint">sys_vars[&#39;SOLVED_MEAN_REWARD&#39;]</code> if the problem is solvable; <code class="prettyprint">max_rewards - (0.10 * (max_rewards-min_rewards))</code> otherwise.</li>
<li><code class="prettyprint">first_solved_epi</code> = first episode that passes the <code class="prettyprint">r_threshold</code>.</li>
<li><code class="prettyprint">mastery_gap</code> = <code class="prettyprint">last_epi - first_solved_epi</code> normally, or <code class="prettyprint">INF</code> to yield stability 0.0 if <code class="prettyprint">first_solved_epi</code> is undefined because no episode passes the threshold.</li>
</ul>

<p>The reasoning for the definition is simple: regardless if the problem is solved or not, we want to see if the agent could stability its performance. Once it solves (for solvable problems) or achieves a sufficiently high score (unsolved problems), its <code class="prettyprint">mean_rewards</code> every episode should not drop too much, even when accounting for random fluctuations. We can define a minimal threshold (10% less than the ideal) that the subsequent episodes should surpass. Then, the stability is simple the ratio of the number of episodes that pass the threshold after the first solution.</p>

<h3 id="consistency">Consistency</h3>

<p><code class="prettyprint">consistency = solved_ratio_of_session</code> for a trial, with range <code class="prettyprint">0.0 - 1.0</code> from inconsistent to consistent. This is the trial-level measurement of stability, as it measures how consistently the agent can solve an environment given multiple repeated sessions.</p>

<p><code class="prettyprint">consistency = 0</code> always for unsolved problems (unbounded rewards) since solution is undefined.</p>

<h3 id="granularity">Granularity</h3>

<p>When <code class="prettyprint">stability=0</code> or <code class="prettyprint">consistency=0</code> the multiplier will drop to zero, regardless if there is any partial solutions. This will make training harder as the agent will not learn from partial solutions if these are treated as non-solutions. So, restore the granularity that preserves partial solution simply by adding 1, i.e. <code class="prettyprint">1+stability</code> and <code class="prettyprint">1+consistency</code>.</p>

<h3 id="amplification">Amplification</h3>

<p>To separate solutions from noise, amplify the good ones and separate them out, while diminish and cluster the worse ones together. Amplify by session-level stability linearly with <code class="prettyprint">*(1+stability)</code> since it&rsquo;s of the same order as <code class="prettyprint">power</code>. Amplify by trial-level stability quadratically with <code class="prettyprint">*(1+consistency)**2</code> since trial stability is of the next order. Amplifier is always positive.</p>

<h3 id="distinguishability">Distinguishability</h3>

<p>Always amplify to make better solutions have more positive fitness score. If <code class="prettyprint">power</code> is negative, amplify toward the 0 axis, i.e. divide by amplifier. If <code class="prettyprint">power</code> is positive, amplify away from the 0 axis, i.e. multiply the amplifier. Essentially, <code class="prettyprint">distinguisher = amplifier**sign(power)</code>.</p>

<h2 id="generalized-metrics">Generalized Metrics</h2>

<p>With the fitness function defined above, we can evaluate a single agent in a single environment. In fact, this is a single experiment with multiple trials, and we compute a fitness score per trial, using the trial&rsquo;s parameter values. We then pick the max fitness score for the Fitness Matrix.</p>

<p>Given that the Lab can run multiple agents across environments in its standardized framework, naturally we ask:</p>

<ul>
<li>we can evaluate <strong>one agent in one environment</strong>, (experiment metrics)</li>
<li>how do we evaluate and compare <strong>multiple agents in one environment</strong>? (environment metrics)</li>
<li>how about <strong>one agent in multiple environments</strong>? (agent metrics)</li>
<li>how about the universal-view of <strong>multiple agents in multiple environments</strong>? (universal metrics)</li>
</ul>

<p>This section formalizes the generalized metrics for these, and shows that the Fitness Matrix is just a max-function projection of some higher dimensional space.</p>

<h3 id="generalization"><a name="generalization"></a>Generalization</h3>

<p>We include the formalization of evaluation metrics across the Agent space and Environment space, and present a generalization, which produces the <a href="#fitness-matrix">Fitness Matrix</a>.</p>

<p><em>We&rsquo;re using LaTeX for better symbolic rendering.</em></p>

<p><img src="images/metric_1.png" alt="Metric 1" />
<img src="images/metric_2.png" alt="Metric 2" />
<img src="images/metric_3.png" alt="Metric 3" />
<img src="images/metric_4.png" alt="Metric 4" /></p>

          <h1 id="algorithms"><a name="algorithms"></a>Algorithms</h1>

<p>The currently implemented algorithms combine deep neural networks with a number of classic reinforcement learning algorithms. These are just a starting point. Please invent your own! </p>

<h2 id="what-is-reinforcement-learning">What is reinforcement learning?</h2>

<p><em>Reinforcement learning (RL) is learning from interaction with an environment, from the consequences of action, rather than from explicit teaching. RL become popular in the 1990s within machine learning and artificial intelligence, but also within operations research and with offshoots in psychology and neuroscience.</em></p>

<p><em>Most RL research is conducted within the mathematical framework of Markov decision processes (MDPs). MDPs involve a decision-making agent interacting with its environment so as to maximize the cumulative reward it receives over time. The agent perceives aspects of the environment&rsquo;s state and selects actions. The agent may estimate a value function and use it to construct better and better decision-making policies over time.</em></p>

<p><em>RL algorithms are methods for solving this kind of problem, that is, problems involving sequences of decisions in which each decision affects what opportunities are available later, in which the effects need not be deterministic, and in which there are long-term goals. RL methods are intended to address the kind of learning and decision making problems that people and animals face in their normal, everyday lives.</em></p>

<p><em>- Rich Sutton</em></p>

<p>For further reading on reinforcement learning see <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver&rsquo;s lectures</a> and the book, <a href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf">Reinforcement Learning: An Introduction, by Sutton and Barto</a>.</p>

<h2 id="why-are-deep-neural-networks-useful-for-solving-rl-problems">Why are deep neural networks useful for solving RL problems?</h2>

<p>RL problems are characterized by incomplete information. The transition probabilities from one state to another given the action taken, for all states and actions are not known. Nor is the distribution of the rewards given a state and action. So in order to solve problems, RL algorithms involve approximating one or more unknown, typically complex, non linear functions. Deep neural networks make good candidates for these function approximators since they excel at approximating complex functions, particularly if the states are characterized by pixel level features.</p>

<p>For further reading on neural networks see <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></p>

<h2 id="terminology">Terminology</h2>

<ul>
<li>Agent: encapsulates a specific algorithm. Each agent has a policy, memory, optimizer, and preprocessor</li>
<li>Online training: agents are trained periodically during episodes</li>
<li>Episodic training: agents are only trained after an episode has completed and before the next episode begins</li>
<li>Policy: rule which determines how to act in a given state, e.g. choose the action A which has the highest Q-value in state <code class="prettyprint">S</code>. May be deterministic or stochatic. </li>
<li>Q function: <code class="prettyprint">Q(S, A)</code>, estimates the value of taking action <code class="prettyprint">A</code> in state <code class="prettyprint">S</code> under a specific policy. </li>
<li>Q-vallue: Value of <code class="prettyprint">Q</code> function for a particular <code class="prettyprint">S</code> and <code class="prettyprint">A</code>.</li>
<li>On policy: the same policy is used to act and evaluate the quality of actions.</li>
<li>Off policy: a different policy is used to act and evaluate the quality of actions. </li>
</ul>

<h2 id="families-of-rl-algorithms"><a name="families"></a>Families of RL Algorithms</h2>

<p>To navigate the different RL algorithms:</p>

<ul>
<li><p>Model-free</p>

<ul>
<li>Value-based

<ul>
<li><strong>DQN</strong></li>
<li><strong>DoubleDQN</strong></li>
<li><strong>Sarsa</strong></li>
<li><strong>OffpolSarsa</strong></li>
</ul></li>
<li>Policy-based

<ul>
<li><strong>REINFORCE</strong></li>
</ul></li>
<li>ActorCritic

<ul>
<li><strong>DPG</strong> - typically implemented with ActorCritic, there&rsquo;s a different policy for exploration and target</li>
<li><strong>DDPG</strong> - very similar to DPG except with target networks and batch normalization</li>
</ul></li>
</ul></li>
<li><p>Model based</p>

<ul>
<li>Value based

<ul>
<li><strong>Dyna</strong></li>
</ul></li>
<li>Policy based

<ul>
<li><em>pending</em></li>
</ul></li>
<li>ActorCritic

<ul>
<li><em>pending</em></li>
</ul></li>
</ul></li>
</ul>

<h2 id="implemented-algorithms">Implemented Algorithms</h2>

<p><em>See the overall list of agents and their implementation status under the <a href="#agents-matrix">Agents Fitness Matrix</a>. This section briefly explains the theories behind the implemented algorithms/agents.</em></p>

<h3 id="q-learning"><a name="q-learning"></a>Q-Learning</h3>

<p>Q-learning algorithms attempt to estimate the optimal Q function, i.e the value of taking action <code class="prettyprint">A</code> in state <code class="prettyprint">S</code> under a specific policy. Q-learning algorithms have an implicit policy, typically <code class="prettyprint">epsilon</code>-greedy in which the action with the maximum <code class="prettyprint">Q</code> value is selected with probability <code class="prettyprint">(1 - epsilon)</code> and a random action is taken with probability <code class="prettyprint">epsilon</code>. The random actions encourage exploration of the state space and help prevent algorithms from getting stuck in local minima.</p>

<p>Q-learning algorithms are off-policy algorithms in that the policy used to evaluate the value of the action taken is different to the policy used to determine which state-action pairs are visited.</p>

<p>It is also a temporal difference algorithm. Updates to the <code class="prettyprint">Q</code> function are based on existing estimates. The estimate in time t is updated using an estimate from time t+1. This allows Q-Learning algorithms to be online and incremental, so the agent can be trained during an episode. The update to <code class="prettyprint">Q_t(S, A)</code> is as follows</p>

<p><img src="images/q_learning.png" alt="Q learning" /></p>

<p>For more details, please see chapter 6 of <a href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf">Reinforcement Learning: An Introduction, Sutton and Barto</a>.</p>

<p>Since the policy that is used to evaluate the target is fixed (a greedy policy that selects the action that maximises the Q-value for a particular state) and is different to the policy used to determine which state-action pairs are being visited, it is possible to use experience replay to train an agent.</p>

<p>This is often needed for Agents to act in environments to get experiences. Experiences consist of the state, the action taken, the next state, and the reward, and are denoted as <code class="prettyprint">&lt;S_t, A_t, R_{t+1}, S_{t+1}&gt;</code>. These experiences are stored in the agents memory. Periodically during an episode the agent is trained. During training n batches of size m are selected from memory and the <code class="prettyprint">Q</code> update step is performed. This is different to Sarsa algorithms which are on-policy and agents are trained after each experience using only the most recent experience.</p>

<p><strong>Deep Q-Learning</strong></p>

<p>Standard Q-learning algorithm with experience replay. Online training every <code class="prettyprint">n</code> experiences.</p>

<p><img src="images/deep_q_learning.png" alt="Deep q learning" /></p>

<p>Agents: </p>

<ul>
<li><code class="prettyprint">DQN</code>: function approximator - feedforward neural network</li>
<li><code class="prettyprint">ConvDQN</code>: function approximator - convolutional network</li>
</ul>

<p><strong>Double Q-Learning</strong></p>

<p>Q-learning algorithm with two <code class="prettyprint">Q</code> function approximators to address the maximisation bias problem, <code class="prettyprint">Q_1</code>, and <code class="prettyprint">Q_2</code>. One <code class="prettyprint">Q</code> function is used to select the action in the next state, <code class="prettyprint">S&#39;</code>, the other is used to evaluate the action in state <code class="prettyprint">S&#39;</code>. Periodically the roles of each <code class="prettyprint">Q</code> function are switched. Online training every <code class="prettyprint">n</code> experiences.</p>

<p><img src="images/double_q_learning.png" alt="Double q learning" /></p>

<p>Agents:</p>

<ul>
<li><code class="prettyprint">DoubleDQN</code>: function approximator - feedforward neural network</li>
<li><code class="prettyprint">DoubleConvQN</code>: function approximator - convolutional network</li>
</ul>

<p><strong>Deep Q-Learning with weight freezing</strong></p>

<p>Deep Q-Learning algorithms tends to be unstable. To address this issue, create two <code class="prettyprint">Q</code> function approximators, one for exploration, <code class="prettyprint">Q_e</code>, and one for evaluating the target, <code class="prettyprint">Q_t</code>. The target is a copy of the exploration network with frozen weights which lag the exploration network.</p>

<p>These weights are updated periodically to match the exploration network. Freezing the target network weights help avoids oscillations in the policy, where slight changes to Q-values can lead to significant changes in the policy, and helps break correlations between the Q-network and the target. See <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Resources_files/deep_rl.pdf">David Silver&rsquo;s</a> lecture slides for more details. Online training every n experiences.</p>

<p><img src="images/freeze_dqn.png" alt="Freeze dqn" /></p>

<p>Agents:</p>

<ul>
<li><code class="prettyprint">FreezeDQN</code>: function approximator - feedforward neural network</li>
</ul>

<h3 id="sarsa"><a name="sarsa"></a>Sarsa</h3>

<p><code class="prettyprint">Sarsa</code> algorithms also attempt to estimate the optimal <code class="prettyprint">Q</code> function. They are on policy algorithms so the policy used to evaluate the target is the same as to the policy used to determine which state-action pairs are being visited.</p>

<p>Like Q-Learning, <code class="prettyprint">Sarsa</code> is a temporal difference algorithm. However, since they are on policy, it is trickier to take advantage of experience replay, requiring storage of the action in state <code class="prettyprint">t+1</code> and the Q-value for the state and action selected in <code class="prettyprint">t+1</code> in an experience. In the following implementations, updates are made after each action with the exception of off policy expected Sarsa.</p>

<p>Sarsa update:</p>

<p><img src="images/sarsa.png" alt="Sarsa" /></p>

<p>This update is made each time the agent acts in an environment and gets an experience <code class="prettyprint">&lt;S_t, A_t, R_{t+1}, S_{t+1}&gt;</code></p>

<p><strong>Sarsa</strong></p>

<p>Standard Sarsa algorithm</p>

<p><img src="images/deep_sarsa.png" alt="Deep sarsa" /></p>

<p>Agents:</p>

<ul>
<li><code class="prettyprint">DeepSarsa</code>: function approximator - feedforward neural network</li>
</ul>

<p><strong>Expected Sarsa</strong></p>

<p>Uses the expected value of the <code class="prettyprint">Q</code> function under the current policy to construct the target instead of the Q-value for the action selected.</p>

<p><img src="images/expected_sarsa.png" alt="Expected sarsa" /></p>

<p>Agents</p>

<ul>
<li><code class="prettyprint">DeepExpectedSarsa</code>: function approximator - feedforward neural network</li>
</ul>

<p><strong>Off Policy Expected Sarsa</strong></p>

<p>Sarsa is typically an on policy algorithm. However, if a different policy is used to evaluate the target than the one used to explore, it becomes and off-policy algorithm. With this set up, Q-Learning can be understood as a specific instance of Off Policy Expected Sarsa, when the policy used to evaluate the target is the greedy policy.</p>

<p>In off policy expected sarsa, actions are selected under the exploration policy, (annealling epsilon greedy for example).  Then the value of next state and action pair is calculated as the expected Q value under the target policy, for example epsilon greedy with some fixed value for epsilon.</p>

<p><code class="prettyprint">Q</code> update and translation to neural network update: Same as <code class="prettyprint">ExpectedSarsa</code> with fixed epsilon.</p>

<p>Agents:</p>

<ul>
<li><code class="prettyprint">OffPolicySarsa</code>: function approximator - feedforward neural network</li>
</ul>

<h3 id="policy-gradient"><a name="pg"></a>Policy Gradient</h3>

<p><em>In progress</em></p>

<h3 id="deep-deterministic-policy-gradient"><a name="ddpg"></a>Deep Deterministic Policy Gradient</h3>

<p><em>In progress</em></p>

<h2 id="rl-theory-resources">RL Theory Resources</h2>

<ul>
<li><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a>: an introductory blog post about deep reinforcement learning; a good and short first intro for new comers.</li>
<li><a href="https://www.youtube.com/watch?v=PtAIh9KSnjo">Deep Reinforcement Learning</a>: 90 minute video overview by John Schulman. Assumes some knowledge of neural networks. If you are not familiar with neural networks, then start with Sutton and Bartos book or David Silver&rsquo;s course</li>
<li><a href="http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf">Reinforcement Learning: An Introduction</a>: Classic textbook by Sutton and Barto. A good place to go next after watching John Schulmans talk. Link is to the in progress 2nd edition. </li>
<li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silvers Reinforcement Learning Course</a></li>
<li><a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning, 2013</a>: The famous paper from DeepMind describing a convolutional neural network Q-Learning architecture which learned to play seven different Atari games, achieving superhuman performance in three of them.</li>
</ul>

          <h1 id="agents"><a name="agents"></a>Agents</h1>

<p>Agents are containers for reinforcement learning algorithms. They consist of a number of different components which are specified in the <code class="prettyprint">experiment_specs</code>.</p>

<blockquote>
<p>The corresponding source code folder. When composing an agent by choosing components, look at the source code folder to see the list of available <code class="prettyprint">Classes</code> to use, and look at their <code class="prettyprint">__init__</code> method to see what parameters are available.</p>
</blockquote>
<pre class="highlight json"><code><span class="s2">"experiment_spec"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/spec/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/spec/problems.json"</span><span class="p">,</span><span class="w">
</span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/agent/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/policy/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/memory/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/optimizer/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/hyperoptimizer/"</span><span class="p">,</span><span class="w">
</span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rl/preprocessor/"</span><span class="p">,</span><span class="w">
</span></code></pre>
<ul>
<li><code class="prettyprint">problem</code>: the gym environment the agent is solving.</li>
<li><code class="prettyprint">Agent</code> (Learning algorithm),: decision function for learning from experiences gained by acting in an environment (eg Q-Learning, Sarsa). This is also the main class for agents. All other components of an agent are contained within this class.</li>
<li><code class="prettyprint">Policy</code>: decision function for acting in an environment. Controls exploration vs. exploitation trade off(e.g. epsilon greedy, boltzmann)</li>
<li><code class="prettyprint">Memory</code>: for storing experiences gained by acting in an environment. Controls how experiences are sampled for an agent to learn from. (e.g. random uniform with no forgetting, prioritized sampling with forgetting)</li>
<li><code class="prettyprint">Optimizer</code>: controls how to optimize the function approximators contained within the agent (e.g. Stochatic Gradient Descent, Adam) </li>
<li><code class="prettyprint">HyperOptimizer</code>: hyperparameter optimization algorithms used to vary the agent parameters and run trials with them (e.g grid search, random search)</li>
<li><code class="prettyprint">PreProcessor</code>: controls the transformations made to state representaions before being passed as inputs to the policy and learning algorithm. (e.g. no preprocessing, concatenating current and previous state). Useful for Atari.</li>
<li><code class="prettyprint">param</code>: the default parameters, unified for all agent components. To know what parameters are available, look into the <code class="prettyprint">__init__</code> method of the components you&rsquo;re using. We have ensured there are no conflicting keys.</li>
<li><code class="prettyprint">param_range</code>: the parameter space specification for the HyperOptimizer to search over in an experiment with many trials.</li>
</ul>

<p>To define an <code class="prettyprint">Agent</code> you must specify each of the components. The example below is from the specification for <code class="prettyprint">dqn</code> in <code class="prettyprint">rl/spec/classic_experiment_specs.json</code>. The <code class="prettyprint">rl/spec/*.json</code> files contains lots of other examples.</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"problem"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"HyperOptimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"GridSearch"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"AdamOptimizer"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"PreProcessor"</span><span class="p">:</span><span class="w"> </span><span class="s2">"NoPreProcessor"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="s2">"param_range"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.001</span><span class="p">,</span><span class="w"> </span><span class="mf">0.005</span><span class="p">,</span><span class="w"> </span><span class="mf">0.01</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02</span><span class="p">],</span><span class="w">
      </span><span class="s2">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.95</span><span class="p">,</span><span class="w"> </span><span class="mf">0.97</span><span class="p">,</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w"> </span><span class="mf">0.999</span><span class="p">],</span><span class="w">
      </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">64</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>Each of the components with the exception of <code class="prettyprint">Agent</code> and <code class="prettyprint">Policy</code> are uncoupled, so can be freely switched in an out for different types of components. Different combinations of components may work better than others. We leave that up to you to experiment with. For some inspiration, see how the Lab users build <a href="#solutions">the best solutions</a>.</p>

<p>For the currently implemented algorithms, the following <code class="prettyprint">Agents</code> can go with the following <code class="prettyprint">Policies</code>.</p>

<ul>
<li><code class="prettyprint">DQN, Sarsa, ExpectedSarsa, OffPolSarsa, FreezeDQN: EpsilonGreedyPolicy, OscillatingEpsilonGreedyPolicy, TargetedEpsilonGreedyPolicy, BoltzmannPolicy</code></li>
<li><code class="prettyprint">DoubleDQN: DoubleDQNPolicy, DoubleDQNBoltzmannPolicy</code></li>
</ul>

<h2 id="problem">problem</h2>

<p><strong>code location:</strong> <code class="prettyprint">rl/spec/problems.json</code></p>

<p>Problems are not part of agent, but they are part of the <code class="prettyprint">experiment_spec</code> that gets specified with the agent.</p>

<p>We have not added all the OpenAI gym environments AKA problems. If you get to new environments using the lab, please add them in <code class="prettyprint">rl/spec/problems.json</code>, and it should be clear from those examples.</p>

<p>Moreover, when adding new problems, consider the dependencies setup too, such as Mujoco. Please add these to the <code class="prettyprint">bin/setup</code> so that other users could run it.</p>

<h2 id="agent">Agent</h2>

<p><strong>code location:</strong> <code class="prettyprint">rl/agent/</code></p>

<p><em>See the overall list of agents and their implementation status under the <a href="#agents-matrix">Agents Fitness Matrix</a>. See <a href="#algorithms">algorithms section</a> for an explanation of the currently implemented agents (learning algorithms).</em></p>

<p>The main <code class="prettyprint">Agent</code> class that will house all the other components. This is where the algorithms like <code class="prettyprint">DQN, DDQN, DDPG</code> are implemented. This class shall contain only the algorithm-specific logic; the generic components are modularized for reuse across different agents.</p>

<p>Despite the multitude of parameters you can see in the contructor of the Agent classes, <code class="prettyprint">Agent</code>s always need the following:</p>

<ul>
<li><code class="prettyprint">gamma</code>: how much to discount the future</li>
<li><code class="prettyprint">hidden_layers</code>: list of hidden layer sizes for neural network function approximators</li>
<li><code class="prettyprint">hidden_layers_activation</code>: activation function for the hidden layers</li>
</ul>

<p>The input and output layer sizes are autoamtically inferred from the environment specs.</p>
<pre class="highlight json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">],</span><span class="w">
    </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<p>For HyperOptimizer to search over the network architecture (yes this is possible), use the <em>auto-architecture</em> mode for building the network. The HyperOptimizer will search over the network depth and width (2 parameters is sufficient to construct a whole network). To do so, set the <code class="prettyprint">auto-architecture</code> param to true, and specific the <code class="prettyprint">num_hidden_layers</code> and the <code class="prettyprint">first_hidden_layer_size</code>.</p>
<pre class="highlight json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Agent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"DQN"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"hidden_layers_activation"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sigmoid"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"auto_architecture"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
    </span><span class="s2">"num_hidden_layers"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">
    </span><span class="s2">"first_hidden_layer_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<h2 id="policy">Policy</h2>

<p><strong>code location:</strong> <code class="prettyprint">rl/policy/</code></p>

<p>A policy is a decision function for acting in an environment. Policies take as input a description of the state space and output an action for the agent to take.</p>

<p>Depending on the algorithm used, agents may directly approximate the policy (policy based algorithms) or have an indirect policy, that depends on the Q-value function approximation (value based algorithms). Algorithms that approximate both the policy and the Q-value function are known as actor-critic algorithms.</p>

<p>All of the algorithms implemented so far are value-based. The policy for acting at each timestep is often a simple epsilon-greedy policy.</p>

<p><img src="images/e_greedy.png" alt="E greedy" /></p>

<p>Alternatively, an indirect policy may use the Q-value to output a probability distribution over actions, and sample actions based on this distribution. This is the approach taken by the Boltzmann policies.</p>

<p>A critical component of the policy is how is balances <em>exploration</em> vs. <em>exploitation</em>. To learn how to act well in an environment an agent must <em>explore</em> the state space. The more random the actions an agent takes, the more it explores. However, to do well in an environment, an agent needs to take the best possible action given the state. It must <em>exploit</em> what it has learnt.</p>

<p>Below is a summary of the currently implemented policies. Each takes a slightly different approach to balancing the exploration-exploitation problem.</p>

<h3 id="epsilongreedypolicy">EpsilonGreedyPolicy</h3>

<p>Parameterized by starting value for epsilon (<code class="prettyprint">init_e</code>), min value for epsilon (<code class="prettyprint">final_e</code>), and the number of epsiodes to anneal epsilon over (<code class="prettyprint">exploration_anneal_episodes</code>). The value of epsilon is decayed linearly from start to min.</p>
<pre class="highlight json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"EpsilonGreedyPolicy"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"init_e"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="s2">"final_e"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">0.1</span><span class="p">,</span><span class="w">
    </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<h3 id="doubledqnpolicy">DoubleDQNPolicy</h3>

<p>When actions are not random this policy selects actions by summing the outputs from each of the two Q-state approximators before taking the max of the result. Same approach as <code class="prettyprint">EpsilonGreedyPolicy</code> to decaying epsilon and same params.</p>

<h3 id="boltzmannpolicy">BoltzmannPolicy</h3>

<p>Parameterized by the starting value for tau (<code class="prettyprint">init_tau</code>), min value for tau (<code class="prettyprint">final_tau</code>), and the number of epsiodes to anneal epsilon over (<code class="prettyprint">exploration_anneal_episodes</code>). At each step this policy selects actions based on the following probability distribution</p>

<p><img src="images/boltzmann.png" alt="Boltzmann" /></p>

<p>Tau is decayed linearly over time in the same way as in the <code class="prettyprint">EpsilonGreedyPolicy</code>.</p>
<pre class="highlight json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Policy"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BoltzmannPolicy"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"init_tau"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="s2">"final_tau"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">0.1</span><span class="p">,</span><span class="w">
    </span><span class="s2">"exploration_anneal_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<h3 id="doubledqnboltzmannpolicy">DoubleDQNBoltzmannPolicy</h3>

<p>Same as the Boltzmann policy except that the Q value used for a given action is the sum of the outputs from each of the two Q-state approximators.</p>

<h3 id="targetedepsilongreedypolicy">TargetedEpsilonGreedyPolicy</h3>

<p>Same params as epsilon greedy policy. This policy swtches between active and inactive exploration cycles controlled by partial mean rewards and it distance to the target mean rewards.</p>

<h3 id="decayingepsilongreedypolicy">DecayingEpsilonGreedyPolicy</h3>

<p>Same params as epsilon greedy policy. Epsilon is decayed exponentially.</p>

<h3 id="oscillatingepsilongreedypolicy">OscillatingEpsilonGreedyPolicy</h3>

<p>Same as epsilon greedy policy except at episode 18 epsilon is dropped to the max of 1/3 or its current value or min epsilon.</p>

<h3 id="creating-your-own">Creating your own</h3>

<p>A policy has to have the following functions. You can create your own by inheriting from Policy or one of its children.</p>
<pre class="highlight python"><code><span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s">'''Returns the action selected given the state'''</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sys_vars</span><span class="p">):</span>
    <span class="s">'''Update value of policy params (e.g. epsilon)
    Called each timestep within an episode'''</span>
</code></pre>
<h2 id="memory">Memory</h2>

<p><strong>code location:</strong> <code class="prettyprint">rl/memory/</code></p>

<p>The agent&rsquo;s memory stores experiences that an agent gains by acting within an environment. An environment is in a particular state. Then the agent acts, and receives a reward from the environment. The agent also receives information about the next state, including a flag indicating whether the next state is the terminal state. Finally, an error measure is stored, indicating how well an agent can estimate the value of this particular transition. </p>

<p>This information about a single step is stored as an experience. Each experience consists of</p>

<ul>
<li>Current state</li>
<li>Action taken</li>
<li>Reward</li>
<li>Next state</li>
<li>Terminal</li>
<li>Error</li>
</ul>
<pre class="highlight python"><code><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminal</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>
</code></pre>
<p>Crucially, the memory controls how long experiences are stored for, and which experiences are sampled from it to use as input into the learning algorithm of an agent. Below is a summary of the currently implemented memories.</p>

<h3 id="linearmemory">LinearMemory</h3>

<p>The size of the memory is unbounded and experiences are sampled random uniformly from memory.</p>

<h3 id="linearmemorywithforgetting">LinearMemoryWithForgetting</h3>
<pre class="highlight json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"LinearMemoryWithForgetting"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"max_mem_len"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<p>Parameterizes by <code class="prettyprint">max_mem_len</code> param which bounds the size of the memory. Once memory reaches the max size, the oldest experiences are deleted from the memory to make space for new experiences. Experiences are sampled random uniformly from memory.</p>

<h3 id="lefttailmemory">LeftTailMemory</h3>

<p>Like linear memory with sampling via a left-tail distribution. This has the effect of drawing more from newer experiences.</p>

<h3 id="prioritizedexperiencereplay">PrioritizedExperienceReplay</h3>

<p>Experiences are weighted by the error, a measure of how well the learning algorithm currently performs on that experience. Experiences are sampled from memory in proportion to the p value (adjusted error value)</p>
<pre class="highlight python"><code><span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">e</span><span class="p">)</span><span class="o">**</span> <span class="n">alpha</span>
</code></pre>
<p>The parameter <code class="prettyprint">e &gt; 0</code> (if not positive, our implementation will bump it up) is a constant added onto the error to prevent experiences with error 0 never being sampled. <code class="prettyprint">alpha</code> controls how spiked the distribution is. The lower <code class="prettyprint">alpha</code> the closer to unform the distribution is. <code class="prettyprint">alpha = 0</code> corresponds to uniform random sampling.</p>
<pre class="highlight json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Memory"</span><span class="p">:</span><span class="w"> </span><span class="s2">"PrioritizedExperienceReplay"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"e"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">0.01</span><span class="p">,</span><span class="w">
    </span><span class="s2">"alpha"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.6</span><span class="p">,</span><span class="w">
    </span><span class="s2">"max_mem_len"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<p>This has the effect of drawing more from experiences that the learning algorithm doesn&rsquo;t perform well on, i.e. the experiences from which is has most to learn. The size of the memory is bounded as in LinearMemoryWithForgetting.</p>

<h3 id="rankedmemory">RankedMemory</h3>

<p>Memory sorted by the <code class="prettyprint">mean_rewards</code> of each episode. Still experimental.</p>

<h3 id="creating-your-own">Creating your own</h3>

<p>A memory has to have the following functions. You can create your own by inheriting from Memory or one of its children.</p>
<pre class="highlight python"><code><span class="k">def</span> <span class="nf">add_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">terminal</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
    <span class="s">'''add an experience to memory'''</span>

<span class="k">def</span> <span class="nf">get_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inds</span><span class="p">):</span>
    <span class="s">'''get a batch of experiences by indices
       helper function called by rand_minibatch'''</span>

<span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">'''get the last experience (batched like get_exp()'''</span>

<span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s">'''returns the size of the memory'''</span>

<span class="k">def</span> <span class="nf">rand_minibatch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="s">'''returns a batch of experiences sampled from memory'''</span>
</code></pre>
<h2 id="optimizer">Optimizer</h2>

<p><strong>code location:</strong> <code class="prettyprint">rl/optimizer/</code></p>

<p>Controls how to optimize the function approximators contained within the agent. For feedforward and convolutional neural networks, we suggest using Adam with the default parameters for everything except the learning rate as this is widely considered to be the best algorithm for optmizing deep neural network based function approximators. For recurrent neural networks we suggest using RMSprop.</p>

<h3 id="sgd">SGD</h3>

<p>Stochastic Gradient Descent. Parameterized by <code class="prettyprint">lr</code> (learning rate), <code class="prettyprint">momentum</code>, <code class="prettyprint">decay</code> and <code class="prettyprint">nestorov</code>. See <a href="https://keras.io/optimizers/#sgd">Keras</a> for more details.</p>
<pre class="highlight json"><code><span class="s2">"dqn"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"Optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"SGDOptimizer"</span><span class="p">,</span><span class="w">
  </span><span class="s2">"param"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w">
    </span><span class="s2">"momentum"</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mf">0.9</span><span class="w">
    </span><span class="s2">"decay"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.00001</span><span class="w">
    </span><span class="s2">"nesterov"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
  </span><span class="p">},</span><span class="w">
</span></code></pre>
<h3 id="adam">Adam</h3>

<p>Parameterized by <code class="prettyprint">lr</code> (learning rate), <code class="prettyprint">beta_1</code>, <code class="prettyprint">beta_2</code>, <code class="prettyprint">epsilon</code>, <code class="prettyprint">decay</code>. See <a href="https://keras.io/optimizers/#adam">Keras</a> for more details.</p>

<h3 id="rmsprop">RMSprop</h3>

<p>Parameterized by <code class="prettyprint">lr</code> (learning rate), <code class="prettyprint">rho</code>, <code class="prettyprint">epsilon</code>, <code class="prettyprint">decay</code>. See <a href="https://keras.io/optimizers/#rmsprop">Keras</a> for more details.</p>

<h2 id="hyperoptimizer"><a name="hyperoptimizer"></a>HyperOptimizer</h2>

<p><strong>code location:</strong> <code class="prettyprint">rl/hyperoptimizer/</code></p>

<p>Controls how to search over the hyperparameter space. We suggest using each of the three hyperoptimizers in the following order when trying to find the optimal parameters for an agent in an environment, to help gradually narrow down the best search space. From rough to fine granularity, <code class="prettyprint">LineSearch &gt; GridSearch &gt; RandomSearch</code>.</p>

<h3 id="linesearch">LineSearch</h3>

<p>Varies one parameter per trial while using default values for the rest. This is useful when trying to estimate of what value range should work for a parameter without having to search through the full space blindly.</p>

<p>Given that we have built a proven base of best parameters, this is now seldom used, as we often could know the optimal parameter values to a small range.</p>

<h3 id="gridsearch">GridSearch</h3>

<p>Once the search space is narrowed down, it&rsquo;s time to do a systematic grid search. This takes the cartesian products of the discrete list of options, i.e. every combination, and run trials. This is the most commonly used, and could help us properly see the heatmap when we probe the search space systematically.</p>

<h3 id="randomsearch">RandomSearch</h3>
<pre class="highlight plaintext"><code>Random Search by sampling on hysphere around a search path:
1. init x a random position in space
2. until termination (max_eval or fitness, e.g. solved all), do:
    2.1 sample new pos some radius away: next_x = x + r
    2.2 if f(next_x) &gt; f(x) then set x = next_x
</code></pre>
<p>After GridSearch, there are usually intermediate values between the grid points that could yield slightly higher fitness score. Narrow down even further to the best search space as shown in the GridSearch heatmap, and do a random search. This is the final fine-tuning. The RandomSearch algorithm is directional and greedy.</p>

<h3 id="hyperoptimizer-roadmap">HyperOptimizer Roadmap</h3>

<p>These are the future hyperparameter optimization algorithms we&rsquo;d like to implement standalone in the Lab. The implementations for them currently exists, but they&rsquo;re too bloated, and their engineering aspects are not ideal for the Lab.</p>

<ul>
<li><a href="https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">TPE</a> / <a href="https://github.com/hyperopt/hyperopt">hyperopt</a></li>
<li><a href="https://github.com/HIPS/Spearmint">Bayesian Optimizer (Spearmint)</a></li>
<li><a href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/#software">SMAC</a></li>
</ul>

<h3 id="implementation-guideline">Implementation Guideline</h3>

<p>All implemented hyperoptimizers shall extend the base <code class="prettyprint">HyperOptimizer</code> class in <code class="prettyprint">rl/hyperoptimizer/base_hyperoptimizer.py</code> and follow its design for compatibility. Below we show this design to be general theoretically and practically. Moreover, do not use bloated dependencies.</p>

<p><strong>Theoretical design:</strong></p>

<p>A hyperoptimizer is a function <code class="prettyprint">h</code> that takes:</p>

<ul>
<li>a trial (objective) function <code class="prettyprint">Trial</code></li>
<li>a parameter space <code class="prettyprint">P</code> (implemented in <code class="prettyprint">experiment_spec</code>)</li>
</ul>

<p>and runs the algorithm:</p>

<ol>
<li>search the next <code class="prettyprint">p</code> in <code class="prettyprint">P</code> using its internal search algorithm, add to its internal <code class="prettyprint">param_search_list</code>.</li>
<li>run a (slow) function <code class="prettyprint">Trial(p) = fitness_score</code> (inside trial data)</li>
<li>update search using the feedback <code class="prettyprint">fitness_score</code></li>
<li>repeat until max steps or fitness condition met</li>
</ol>

<p>Note that the search space <code class="prettyprint">P</code> is a tensor space product of <code class="prettyprint">m</code> bounded real spaces <code class="prettyprint">R</code> and <code class="prettyprint">n</code> bounded discrete spaces <code class="prettyprint">N</code>. The search path in <code class="prettyprint">param_search_list</code> must also be well-ordered to ensure resumability.</p>

<p><strong>Implementation requirements:</strong></p>

<ol>
<li>we want order-preserving and persistence in search for the ability to resume/reproduce an experiment.</li>
<li>the search algorithm may have its own internal memory/belief to facilitate search.</li>
<li>the <code class="prettyprint">Trial</code> function shall be treated as a blackbox <code class="prettyprint">Trial(p) = fitness_score</code> with input/output <code class="prettyprint">(p, fitness_score)</code> for the generality of implementation/</li>
</ol>

<p><strong>Specification of search space:</strong></p>

<p>1. for real variable, specify a distribution (an interval is just a uniformly distributed space) in the <code class="prettyprint">experiment_spec.param_range</code>. Example:</p>
<pre class="highlight json"><code><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="s2">"min"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0005</span><span class="p">,</span><span class="w">
  </span><span class="s2">"max"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.05</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>
<p>2. for discrete variable, specify a list of the values to search over (since it is finite anyway) in the <code class="prettyprint">experiment_spec.param_range</code>. This will automatically be sorted when read into <code class="prettyprint">HyperOptimizer</code> to ensure ordering. Example:</p>
<pre class="highlight json"><code><span class="s2">"lr"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="w"> </span><span class="mf">0.02</span><span class="p">,</span><span class="w"> </span><span class="mf">0.05</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="mf">0.2</span><span class="p">]</span><span class="w">
</span></code></pre>
<p>The hyperopt implementation shall be able to take these 2 types of param_range specs and construct its search space.</p>

<p>Note that whether a variable is real or discrete can be up to the user; some variable such as <code class="prettyprint">lr</code> can be sampled from interval <code class="prettyprint">0.001 to 0.1</code> or human-specified options <code class="prettyprint">[0.01, 0.02, 0.05, 0.1, 0.2]</code>. One way may be more efficient than the other depending on the search algorithm.</p>

<p>The experiment will run it as:</p>
<pre class="highlight python"><code><span class="c"># specify which hyperoptimizer class to use in spec for bookkeeping</span>
<span class="n">Hopt</span> <span class="o">=</span> <span class="n">get_module</span><span class="p">(</span><span class="n">GREF</span><span class="p">,</span> <span class="n">experiment_spec</span><span class="p">[</span><span class="s">'HyperOptimizer'</span><span class="p">])</span>
<span class="n">hopt</span> <span class="o">=</span> <span class="n">Hopt</span><span class="p">(</span><span class="n">Trial</span><span class="p">,</span> <span class="o">**</span><span class="n">experiment_kwargs</span><span class="p">)</span>
<span class="n">experiment_data</span> <span class="o">=</span> <span class="n">hopt</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</code></pre>
<h2 id="preprocessor">Preprocessor</h2>

<p><strong>code location:</strong> <code class="prettyprint">rl/preprocessor/</code></p>

<p>Sometimes preprocessing the states before they are received by the agent can help to simplify the problem or make the agent strong. One example is the pixel preprocessing the removes color channels and rescales image size, in order to reduce unnecessary information overload. The other is to concat states from sequential timesteps to present richer, correlated information that is otherwise sparse.</p>

<p>The change in dimensions after preprocessing is handled automatically, so you can use them without any concerns.</p>

<h3 id="nopreprocessor">NoPreProcessor</h3>

<p>The default that does not preprocess, but pass on the states as is.</p>

<h3 id="stackstates">StackStates</h3>

<p>Concat the current and the previous states. Turns out this boosts agent performance in the <code class="prettyprint">LunarLander-v2</code> environment.</p>

<h3 id="diffstates">DiffStates</h3>

<p>Take the difference <code class="prettyprint">new_states - old_states</code>.</p>

<h3 id="atari">Atari</h3>

<p>Convert images to greyscale, downsize, crop, then stack 4 most recent states together. Useful for the Atari environments.</p>

          <h1 id="development"><a name="development"></a>Development</h1>

<p>For agent-specific development, see <a href="#agents">Agents</a>. This section details the general, non-agent development guideline.</p>

<p>The design of the code is clean enough to simply infer how things work by existing examples. The fastest way is to develop is to dig into the source code.</p>

<ul>
<li><code class="prettyprint">data/</code>: data folders grouped per experiment, each of which contains all the graphs per trial sessions, JSON data file per trial, and csv metrics dataframe per run of multiple trials</li>
<li><code class="prettyprint">rl/agent/</code>: custom agents. Refer to <code class="prettyprint">base_agent.py</code> and <code class="prettyprint">dqn.py</code> to build your own</li>
<li><code class="prettyprint">rl/hyperoptimizer/</code>: Hyperparameter optimizers for the Experiments</li>
<li><code class="prettyprint">rl/memory/</code>: RL agent memory classes</li>
<li><code class="prettyprint">rl/optimizer/</code>: RL agent NN optimizer classes</li>
<li><code class="prettyprint">rl/policy/</code>: RL agent policy classes</li>
<li><code class="prettyprint">rl/preprocessor/</code>: RL agent preprocessor (state and memory) classes</li>
<li><code class="prettyprint">rl/spec/</code>: specify new problems and experiment_specs to run experiments for</li>
<li><code class="prettyprint">rl/spec/component_locks.json</code>: locks to check RL component combination in specs</li>
<li><code class="prettyprint">rl/analytics.py</code>: the data analytics module for output experiment data</li>
<li><code class="prettyprint">rl/experiment.py</code>: the main high level experiment logic</li>
<li><code class="prettyprint">rl/util.py</code>: Generic util</li>
</ul>

<aside class="notice">
As the Lab grows, we will add more development guide as needed.
</aside>

          <h1 id="roadmap"><a name="roadmap"></a>Roadmap</h1>

<p>Check the latest under the <a href="https://github.com/kengz/openai_lab/projects">Github Projects</a></p>

<h2 id="motivations"><a name="motivations"></a>Motivations</h2>

<p><em>This section is more casual, but we thought we&rsquo;d share the motivations behind the Lab.</em></p>

<p>We the authors never set out to build OpenAI Lab with any grand vision in mind. We just wanted to test our RL ideas in OpenAI Gym, faced many problems along the way, and their solutions became features. These opened up new adjacent possibles to do new things, and even more problems, and so on. Before we knew it, the critical components fell it place and we had something very similar to a scientific lab.</p>

<p>The problems faced by us are numerous and diverse, but there are several major categories. The first two are nicely described by WildML&rsquo;s Denny in his post <a href="http://blog.dennybritz.com/2017/01/17/engineering-is-the-bottleneck-in-deep-learning-research/">Engineering Is The Bottleneck In (Deep Learning) Research</a>, which resonates strongly with a lot of people.</p>

<p><strong>1. the difficulty of building upon other&rsquo;s work</strong></p>

<p>If you have tried to implement any algorithms by looking at someone elses code, chances are it&rsquo;s painful. Sometimes you just want to research a small component like a prioritized memory, but you&rsquo;d have to write 90% of the unrelated components from scratch. Simply look at the solution source codes submitted to the OpenAI Gym leaderboard; you can&rsquo;t extend them to build something much bigger.</p>

<p>Of many implementations we saw which solve OpenAI gym environments, many had to rewrite the same basic components instead of just the new components being researched. This is unnecessary and inefficient.</p>

<p>There is no design or engineering standards for reinforcement learning, and that contributes to the major inertia in RL research. A lot of times research ideas are not difficult to come by, but implementing them is hard because there is <em>no reliable foundation to build on</em>.</p>

<p>We patiently built every piece of that foundation because giving up wasn&rsquo;t an option, so here it is. As the Lab grows, we hope that engineers and researchers can experiment with an idea fast by building on top of our existing components, and of course, contribute back.</p>

<p><strong>2. the lack of rigor in comparisons</strong></p>

<p>Denny describes this already, <a href="http://blog.dennybritz.com/2017/01/17/engineering-is-the-bottleneck-in-deep-learning-research/">read his blog</a>.</p>

<p>As the Lab became mature, we became more ambitious and try to solve more environment, with more agents. This naturally begs the question, &ldquo;how do we compare them, across agents and environments?&rdquo;</p>

<p>Multiple experiments running in the Lab will produce standardized data analytics and evaluation metrics. This will allow us to compare agents and environments meaningfully, and that is the point of the Lab&rsquo;s <a href="#fitness-matrix">Fitness Matrix</a>. It also inspired a <a href="#metrics">generalization of evaluation metrics</a>, which we have only discovered recently.</p>

<p><strong>3. the inertia to high level vision</strong></p>

<p>When you&rsquo;re heels down implementing an algorithm and the extra 90% side components from scratch, it&rsquo;s hard to organize your work from a high level. Having to worry about other irrelevant components also makes you lose focus. The Lab removes that inertia and frees us from that myopic vision.</p>

<p>This freedom means more mental energy and time to focus on the essential components of research. It opens up new adjacent possibles and has us asking new questions.</p>

<p><strong>The New Adjacent Possibles</strong></p>

<p>With those problems above resolved, the Lab opens up the new adjacent possibles and allows us to do more. Below are some:</p>

<ul>
<li>independent discovery of RankedMemory, which is essentially the Priorized Experience Replay (PER) idea.</li>
<li>the study of hyperparameters at scale, with the <a href="#analysis">Analysis Graph</a>.</li>
<li>an explosion of experiments; suddenly we have the ability to test a lot of ideas quickly.</li>
<li>new high level, powerful visuals from the <a href="#analysis">analysis graphs</a> to eyeball the performance and potentials of an agent.</li>
<li>auto-architecture of agent&rsquo;s neural net, because we wanted to study some variables of the NN architecture as hyperparameters.</li>
<li>the perspective of RL as experimental science, and experimenting as breeding agents in the environments.</li>
<li>the design of a new <a href="#fitness"><code class="prettyprint">fitness_score</code> that provides richer evaluation metrics</a>, which considers solution speed, stability, consistency, and much more.</li>
<li>the <a href="#fitness-matrix">Fitness Matrix</a> and the ability to compare fitness_score across its grid.</li>
<li>the <a href="#generalization">generalization of evaluation metrics</a>, which casts the hyperoptimization problem as one of temperature fields and differential calculus. Also gives the Fitness Fields Matrix.</li>
</ul>

          <h1 id="contributing"><a name="contributing"></a>Contributing</h1>

<ul>
<li>Invented a new algorithm/added some features? Submit a <a href="https://github.com/kengz/openai_lab/pulls">Pull Request here</a>.</li>
<li>Created some stronger agents that beats the current results? Read <a href="#solutions">here to submit new solution</a>.</li>
<li>Wanna just talk? Hit us up on Twitter below.</li>
</ul>

<h3 id="lab-contributors">Lab Contributors</h3>

<p>See the <a href="https://github.com/kengz/openai_lab/graphs/contributors">full list of contributors here</a>.</p>

<h2 id="authors">Authors</h2>

<p><em>Note: we are not affiliated with OpenAI; OpenAI Lab is not tied to any organizations.</em></p>

<ul>
<li>Wah Loon Keng. <a href="https://twitter.com/kengzwl">twitter: @kengzwl</a></li>
<li>Laura Graesser. <a href="https://twitter.com/lgraesser3">twitter: @lgraesser3</a></li>
</ul>

<p><em>OpenAI we hope you find this useful!</em></p>

      </div>
      <div class="dark-box">
      </div>
    </div>
  </body>
</html>
